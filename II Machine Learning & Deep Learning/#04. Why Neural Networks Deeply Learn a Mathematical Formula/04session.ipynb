{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"+5\">#04. Why Neural Networks Deeply Learn a Mathematical Formula?</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Book + Private Lessons [Here ‚Üó](https://sotastica.com/reservar)\n",
    "- Subscribe to my [Blog ‚Üó](https://blog.pythonassembly.com/)\n",
    "- Let's keep in touch on [LinkedIn ‚Üó](www.linkedin.com/in/jsulopz) üòÑ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Machine Learning, what does it mean?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - The Machine Learns...\n",
    ">\n",
    "> But, **what does it learn?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<blockquote class=\"twitter-tweet\" data-lang=\"en\"><p lang=\"en\" dir=\"ltr\">Machine Learning, what does it mean? ‚èØ<br><br>¬∑ The machine learns...<br><br>Ha ha, not funny! ü§® What does it learn?<br><br>¬∑ A mathematical equation. For example: <a href=\"https://t.co/sjtq9F2pq7\">pic.twitter.com/sjtq9F2pq7</a></p>&mdash; Jes√∫s L√≥pez (@sotastica) <a href=\"https://twitter.com/sotastica/status/1449735653328031745?ref_src=twsrc%5Etfw\">October 17, 2021</a></blockquote> <script async src=\"https://platform.twitter.com/widgets.js\" charset=\"utf-8\"></script>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%HTML\n",
    "<blockquote class=\"twitter-tweet\" data-lang=\"en\"><p lang=\"en\" dir=\"ltr\">Machine Learning, what does it mean? ‚èØ<br><br>¬∑ The machine learns...<br><br>Ha ha, not funny! ü§® What does it learn?<br><br>¬∑ A mathematical equation. For example: <a href=\"https://t.co/sjtq9F2pq7\">pic.twitter.com/sjtq9F2pq7</a></p>&mdash; Jes√∫s L√≥pez (@sotastica) <a href=\"https://twitter.com/sotastica/status/1449735653328031745?ref_src=twsrc%5Etfw\">October 17, 2021</a></blockquote> <script async src=\"https://platform.twitter.com/widgets.js\" charset=\"utf-8\"></script>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How does the Machine Learn?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In a Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/Ht3rYS-JilE\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%HTML\n",
    "<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/Ht3rYS-JilE\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In a Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/IHZwWFHWa-w?start=329\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%HTML\n",
    "<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/IHZwWFHWa-w?start=329\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A Practical Example ‚Üí [Tesla Autopilot](https://www.tesla.com/AI)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An Example where It Fails ‚Üí [Tesla Confuses Moon with Semaphore](https://twitter.com/Carnage4Life/status/1418920100086784000?s=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - Simply execute the following lines of code to load the data.\n",
    "> - This dataset contains **statistics about Car Accidents** (columns)\n",
    "> - In each one of **USA States** (rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.kaggle.com/fivethirtyeight/fivethirtyeight-bad-drivers-dataset/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>total</th>\n",
       "      <th>speeding</th>\n",
       "      <th>alcohol</th>\n",
       "      <th>not_distracted</th>\n",
       "      <th>no_previous</th>\n",
       "      <th>ins_premium</th>\n",
       "      <th>ins_losses</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>abbrev</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>WY</th>\n",
       "      <td>17.4</td>\n",
       "      <td>7.308</td>\n",
       "      <td>5.568</td>\n",
       "      <td>14.094</td>\n",
       "      <td>15.660</td>\n",
       "      <td>791.14</td>\n",
       "      <td>122.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MI</th>\n",
       "      <td>14.1</td>\n",
       "      <td>3.384</td>\n",
       "      <td>3.948</td>\n",
       "      <td>13.395</td>\n",
       "      <td>10.857</td>\n",
       "      <td>1110.61</td>\n",
       "      <td>152.26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MO</th>\n",
       "      <td>16.1</td>\n",
       "      <td>6.923</td>\n",
       "      <td>5.474</td>\n",
       "      <td>14.812</td>\n",
       "      <td>13.524</td>\n",
       "      <td>790.32</td>\n",
       "      <td>144.45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FL</th>\n",
       "      <td>17.9</td>\n",
       "      <td>3.759</td>\n",
       "      <td>5.191</td>\n",
       "      <td>16.468</td>\n",
       "      <td>16.826</td>\n",
       "      <td>1160.13</td>\n",
       "      <td>144.18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>WA</th>\n",
       "      <td>10.6</td>\n",
       "      <td>4.452</td>\n",
       "      <td>3.498</td>\n",
       "      <td>8.692</td>\n",
       "      <td>9.116</td>\n",
       "      <td>890.03</td>\n",
       "      <td>111.62</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        total  speeding  alcohol  not_distracted  no_previous  ins_premium  \\\n",
       "abbrev                                                                       \n",
       "WY       17.4     7.308    5.568          14.094       15.660       791.14   \n",
       "MI       14.1     3.384    3.948          13.395       10.857      1110.61   \n",
       "MO       16.1     6.923    5.474          14.812       13.524       790.32   \n",
       "FL       17.9     3.759    5.191          16.468       16.826      1160.13   \n",
       "WA       10.6     4.452    3.498           8.692        9.116       890.03   \n",
       "\n",
       "        ins_losses  \n",
       "abbrev              \n",
       "WY          122.04  \n",
       "MI          152.26  \n",
       "MO          144.45  \n",
       "FL          144.18  \n",
       "WA          111.62  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "df = sns.load_dataset(name='car_crashes', index_col='abbrev')\n",
    "df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "model = LinearRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>speeding</th>\n",
       "      <th>alcohol</th>\n",
       "      <th>not_distracted</th>\n",
       "      <th>no_previous</th>\n",
       "      <th>ins_premium</th>\n",
       "      <th>ins_losses</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>abbrev</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>AL</th>\n",
       "      <td>7.332</td>\n",
       "      <td>5.640</td>\n",
       "      <td>18.048</td>\n",
       "      <td>15.040</td>\n",
       "      <td>784.55</td>\n",
       "      <td>145.08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AK</th>\n",
       "      <td>7.421</td>\n",
       "      <td>4.525</td>\n",
       "      <td>16.290</td>\n",
       "      <td>17.014</td>\n",
       "      <td>1053.48</td>\n",
       "      <td>133.93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AZ</th>\n",
       "      <td>6.510</td>\n",
       "      <td>5.208</td>\n",
       "      <td>15.624</td>\n",
       "      <td>17.856</td>\n",
       "      <td>899.47</td>\n",
       "      <td>110.35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AR</th>\n",
       "      <td>4.032</td>\n",
       "      <td>5.824</td>\n",
       "      <td>21.056</td>\n",
       "      <td>21.280</td>\n",
       "      <td>827.34</td>\n",
       "      <td>142.39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CA</th>\n",
       "      <td>4.200</td>\n",
       "      <td>3.360</td>\n",
       "      <td>10.920</td>\n",
       "      <td>10.680</td>\n",
       "      <td>878.41</td>\n",
       "      <td>165.63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CO</th>\n",
       "      <td>5.032</td>\n",
       "      <td>3.808</td>\n",
       "      <td>10.744</td>\n",
       "      <td>12.920</td>\n",
       "      <td>835.50</td>\n",
       "      <td>139.91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CT</th>\n",
       "      <td>4.968</td>\n",
       "      <td>3.888</td>\n",
       "      <td>9.396</td>\n",
       "      <td>8.856</td>\n",
       "      <td>1068.73</td>\n",
       "      <td>167.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DE</th>\n",
       "      <td>6.156</td>\n",
       "      <td>4.860</td>\n",
       "      <td>14.094</td>\n",
       "      <td>16.038</td>\n",
       "      <td>1137.87</td>\n",
       "      <td>151.48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DC</th>\n",
       "      <td>2.006</td>\n",
       "      <td>1.593</td>\n",
       "      <td>5.900</td>\n",
       "      <td>5.900</td>\n",
       "      <td>1273.89</td>\n",
       "      <td>136.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FL</th>\n",
       "      <td>3.759</td>\n",
       "      <td>5.191</td>\n",
       "      <td>16.468</td>\n",
       "      <td>16.826</td>\n",
       "      <td>1160.13</td>\n",
       "      <td>144.18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GA</th>\n",
       "      <td>2.964</td>\n",
       "      <td>3.900</td>\n",
       "      <td>14.820</td>\n",
       "      <td>14.508</td>\n",
       "      <td>913.15</td>\n",
       "      <td>142.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HI</th>\n",
       "      <td>9.450</td>\n",
       "      <td>7.175</td>\n",
       "      <td>14.350</td>\n",
       "      <td>15.225</td>\n",
       "      <td>861.18</td>\n",
       "      <td>120.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ID</th>\n",
       "      <td>5.508</td>\n",
       "      <td>4.437</td>\n",
       "      <td>13.005</td>\n",
       "      <td>14.994</td>\n",
       "      <td>641.96</td>\n",
       "      <td>82.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>IL</th>\n",
       "      <td>4.608</td>\n",
       "      <td>4.352</td>\n",
       "      <td>12.032</td>\n",
       "      <td>12.288</td>\n",
       "      <td>803.11</td>\n",
       "      <td>139.15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>IN</th>\n",
       "      <td>3.625</td>\n",
       "      <td>4.205</td>\n",
       "      <td>13.775</td>\n",
       "      <td>13.775</td>\n",
       "      <td>710.46</td>\n",
       "      <td>108.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>IA</th>\n",
       "      <td>2.669</td>\n",
       "      <td>3.925</td>\n",
       "      <td>15.229</td>\n",
       "      <td>13.659</td>\n",
       "      <td>649.06</td>\n",
       "      <td>114.47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>KS</th>\n",
       "      <td>4.806</td>\n",
       "      <td>4.272</td>\n",
       "      <td>13.706</td>\n",
       "      <td>15.130</td>\n",
       "      <td>780.45</td>\n",
       "      <td>133.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>KY</th>\n",
       "      <td>4.066</td>\n",
       "      <td>4.922</td>\n",
       "      <td>16.692</td>\n",
       "      <td>16.264</td>\n",
       "      <td>872.51</td>\n",
       "      <td>137.13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LA</th>\n",
       "      <td>7.175</td>\n",
       "      <td>6.765</td>\n",
       "      <td>14.965</td>\n",
       "      <td>20.090</td>\n",
       "      <td>1281.55</td>\n",
       "      <td>194.78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ME</th>\n",
       "      <td>5.738</td>\n",
       "      <td>4.530</td>\n",
       "      <td>13.137</td>\n",
       "      <td>12.684</td>\n",
       "      <td>661.88</td>\n",
       "      <td>96.57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MD</th>\n",
       "      <td>4.250</td>\n",
       "      <td>4.000</td>\n",
       "      <td>8.875</td>\n",
       "      <td>12.375</td>\n",
       "      <td>1048.78</td>\n",
       "      <td>192.70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MA</th>\n",
       "      <td>1.886</td>\n",
       "      <td>2.870</td>\n",
       "      <td>7.134</td>\n",
       "      <td>6.560</td>\n",
       "      <td>1011.14</td>\n",
       "      <td>135.63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MI</th>\n",
       "      <td>3.384</td>\n",
       "      <td>3.948</td>\n",
       "      <td>13.395</td>\n",
       "      <td>10.857</td>\n",
       "      <td>1110.61</td>\n",
       "      <td>152.26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MN</th>\n",
       "      <td>2.208</td>\n",
       "      <td>2.784</td>\n",
       "      <td>8.448</td>\n",
       "      <td>8.448</td>\n",
       "      <td>777.18</td>\n",
       "      <td>133.35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MS</th>\n",
       "      <td>2.640</td>\n",
       "      <td>5.456</td>\n",
       "      <td>1.760</td>\n",
       "      <td>17.600</td>\n",
       "      <td>896.07</td>\n",
       "      <td>155.77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MO</th>\n",
       "      <td>6.923</td>\n",
       "      <td>5.474</td>\n",
       "      <td>14.812</td>\n",
       "      <td>13.524</td>\n",
       "      <td>790.32</td>\n",
       "      <td>144.45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MT</th>\n",
       "      <td>8.346</td>\n",
       "      <td>9.416</td>\n",
       "      <td>17.976</td>\n",
       "      <td>18.190</td>\n",
       "      <td>816.21</td>\n",
       "      <td>85.15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NE</th>\n",
       "      <td>1.937</td>\n",
       "      <td>5.215</td>\n",
       "      <td>13.857</td>\n",
       "      <td>13.410</td>\n",
       "      <td>732.28</td>\n",
       "      <td>114.82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NV</th>\n",
       "      <td>5.439</td>\n",
       "      <td>4.704</td>\n",
       "      <td>13.965</td>\n",
       "      <td>14.553</td>\n",
       "      <td>1029.87</td>\n",
       "      <td>138.71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NH</th>\n",
       "      <td>4.060</td>\n",
       "      <td>3.480</td>\n",
       "      <td>10.092</td>\n",
       "      <td>9.628</td>\n",
       "      <td>746.54</td>\n",
       "      <td>120.21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NJ</th>\n",
       "      <td>1.792</td>\n",
       "      <td>3.136</td>\n",
       "      <td>9.632</td>\n",
       "      <td>8.736</td>\n",
       "      <td>1301.52</td>\n",
       "      <td>159.85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NM</th>\n",
       "      <td>3.496</td>\n",
       "      <td>4.968</td>\n",
       "      <td>12.328</td>\n",
       "      <td>18.032</td>\n",
       "      <td>869.85</td>\n",
       "      <td>120.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NY</th>\n",
       "      <td>3.936</td>\n",
       "      <td>3.567</td>\n",
       "      <td>10.824</td>\n",
       "      <td>9.840</td>\n",
       "      <td>1234.31</td>\n",
       "      <td>150.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NC</th>\n",
       "      <td>6.552</td>\n",
       "      <td>5.208</td>\n",
       "      <td>15.792</td>\n",
       "      <td>13.608</td>\n",
       "      <td>708.24</td>\n",
       "      <td>127.82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ND</th>\n",
       "      <td>5.497</td>\n",
       "      <td>10.038</td>\n",
       "      <td>23.661</td>\n",
       "      <td>20.554</td>\n",
       "      <td>688.75</td>\n",
       "      <td>109.72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>OH</th>\n",
       "      <td>3.948</td>\n",
       "      <td>4.794</td>\n",
       "      <td>13.959</td>\n",
       "      <td>11.562</td>\n",
       "      <td>697.73</td>\n",
       "      <td>133.52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>OK</th>\n",
       "      <td>6.368</td>\n",
       "      <td>5.771</td>\n",
       "      <td>18.308</td>\n",
       "      <td>18.706</td>\n",
       "      <td>881.51</td>\n",
       "      <td>178.86</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>OR</th>\n",
       "      <td>4.224</td>\n",
       "      <td>3.328</td>\n",
       "      <td>8.576</td>\n",
       "      <td>11.520</td>\n",
       "      <td>804.71</td>\n",
       "      <td>104.61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PA</th>\n",
       "      <td>9.100</td>\n",
       "      <td>5.642</td>\n",
       "      <td>17.472</td>\n",
       "      <td>16.016</td>\n",
       "      <td>905.99</td>\n",
       "      <td>153.86</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RI</th>\n",
       "      <td>3.774</td>\n",
       "      <td>4.218</td>\n",
       "      <td>10.212</td>\n",
       "      <td>8.769</td>\n",
       "      <td>1148.99</td>\n",
       "      <td>148.58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SC</th>\n",
       "      <td>9.082</td>\n",
       "      <td>9.799</td>\n",
       "      <td>22.944</td>\n",
       "      <td>19.359</td>\n",
       "      <td>858.97</td>\n",
       "      <td>116.29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SD</th>\n",
       "      <td>6.014</td>\n",
       "      <td>6.402</td>\n",
       "      <td>19.012</td>\n",
       "      <td>16.684</td>\n",
       "      <td>669.31</td>\n",
       "      <td>96.87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TN</th>\n",
       "      <td>4.095</td>\n",
       "      <td>5.655</td>\n",
       "      <td>15.990</td>\n",
       "      <td>15.795</td>\n",
       "      <td>767.91</td>\n",
       "      <td>155.57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TX</th>\n",
       "      <td>7.760</td>\n",
       "      <td>7.372</td>\n",
       "      <td>17.654</td>\n",
       "      <td>16.878</td>\n",
       "      <td>1004.75</td>\n",
       "      <td>156.83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>UT</th>\n",
       "      <td>4.859</td>\n",
       "      <td>1.808</td>\n",
       "      <td>9.944</td>\n",
       "      <td>10.848</td>\n",
       "      <td>809.38</td>\n",
       "      <td>109.48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>VT</th>\n",
       "      <td>4.080</td>\n",
       "      <td>4.080</td>\n",
       "      <td>13.056</td>\n",
       "      <td>12.920</td>\n",
       "      <td>716.20</td>\n",
       "      <td>109.61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>VA</th>\n",
       "      <td>2.413</td>\n",
       "      <td>3.429</td>\n",
       "      <td>11.049</td>\n",
       "      <td>11.176</td>\n",
       "      <td>768.95</td>\n",
       "      <td>153.72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>WA</th>\n",
       "      <td>4.452</td>\n",
       "      <td>3.498</td>\n",
       "      <td>8.692</td>\n",
       "      <td>9.116</td>\n",
       "      <td>890.03</td>\n",
       "      <td>111.62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>WV</th>\n",
       "      <td>8.092</td>\n",
       "      <td>6.664</td>\n",
       "      <td>23.086</td>\n",
       "      <td>20.706</td>\n",
       "      <td>992.61</td>\n",
       "      <td>152.56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>WI</th>\n",
       "      <td>4.968</td>\n",
       "      <td>4.554</td>\n",
       "      <td>5.382</td>\n",
       "      <td>11.592</td>\n",
       "      <td>670.31</td>\n",
       "      <td>106.62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>WY</th>\n",
       "      <td>7.308</td>\n",
       "      <td>5.568</td>\n",
       "      <td>14.094</td>\n",
       "      <td>15.660</td>\n",
       "      <td>791.14</td>\n",
       "      <td>122.04</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        speeding  alcohol  not_distracted  no_previous  ins_premium  \\\n",
       "abbrev                                                                \n",
       "AL         7.332    5.640          18.048       15.040       784.55   \n",
       "AK         7.421    4.525          16.290       17.014      1053.48   \n",
       "AZ         6.510    5.208          15.624       17.856       899.47   \n",
       "AR         4.032    5.824          21.056       21.280       827.34   \n",
       "CA         4.200    3.360          10.920       10.680       878.41   \n",
       "CO         5.032    3.808          10.744       12.920       835.50   \n",
       "CT         4.968    3.888           9.396        8.856      1068.73   \n",
       "DE         6.156    4.860          14.094       16.038      1137.87   \n",
       "DC         2.006    1.593           5.900        5.900      1273.89   \n",
       "FL         3.759    5.191          16.468       16.826      1160.13   \n",
       "GA         2.964    3.900          14.820       14.508       913.15   \n",
       "HI         9.450    7.175          14.350       15.225       861.18   \n",
       "ID         5.508    4.437          13.005       14.994       641.96   \n",
       "IL         4.608    4.352          12.032       12.288       803.11   \n",
       "IN         3.625    4.205          13.775       13.775       710.46   \n",
       "IA         2.669    3.925          15.229       13.659       649.06   \n",
       "KS         4.806    4.272          13.706       15.130       780.45   \n",
       "KY         4.066    4.922          16.692       16.264       872.51   \n",
       "LA         7.175    6.765          14.965       20.090      1281.55   \n",
       "ME         5.738    4.530          13.137       12.684       661.88   \n",
       "MD         4.250    4.000           8.875       12.375      1048.78   \n",
       "MA         1.886    2.870           7.134        6.560      1011.14   \n",
       "MI         3.384    3.948          13.395       10.857      1110.61   \n",
       "MN         2.208    2.784           8.448        8.448       777.18   \n",
       "MS         2.640    5.456           1.760       17.600       896.07   \n",
       "MO         6.923    5.474          14.812       13.524       790.32   \n",
       "MT         8.346    9.416          17.976       18.190       816.21   \n",
       "NE         1.937    5.215          13.857       13.410       732.28   \n",
       "NV         5.439    4.704          13.965       14.553      1029.87   \n",
       "NH         4.060    3.480          10.092        9.628       746.54   \n",
       "NJ         1.792    3.136           9.632        8.736      1301.52   \n",
       "NM         3.496    4.968          12.328       18.032       869.85   \n",
       "NY         3.936    3.567          10.824        9.840      1234.31   \n",
       "NC         6.552    5.208          15.792       13.608       708.24   \n",
       "ND         5.497   10.038          23.661       20.554       688.75   \n",
       "OH         3.948    4.794          13.959       11.562       697.73   \n",
       "OK         6.368    5.771          18.308       18.706       881.51   \n",
       "OR         4.224    3.328           8.576       11.520       804.71   \n",
       "PA         9.100    5.642          17.472       16.016       905.99   \n",
       "RI         3.774    4.218          10.212        8.769      1148.99   \n",
       "SC         9.082    9.799          22.944       19.359       858.97   \n",
       "SD         6.014    6.402          19.012       16.684       669.31   \n",
       "TN         4.095    5.655          15.990       15.795       767.91   \n",
       "TX         7.760    7.372          17.654       16.878      1004.75   \n",
       "UT         4.859    1.808           9.944       10.848       809.38   \n",
       "VT         4.080    4.080          13.056       12.920       716.20   \n",
       "VA         2.413    3.429          11.049       11.176       768.95   \n",
       "WA         4.452    3.498           8.692        9.116       890.03   \n",
       "WV         8.092    6.664          23.086       20.706       992.61   \n",
       "WI         4.968    4.554           5.382       11.592       670.31   \n",
       "WY         7.308    5.568          14.094       15.660       791.14   \n",
       "\n",
       "        ins_losses  \n",
       "abbrev              \n",
       "AL          145.08  \n",
       "AK          133.93  \n",
       "AZ          110.35  \n",
       "AR          142.39  \n",
       "CA          165.63  \n",
       "CO          139.91  \n",
       "CT          167.02  \n",
       "DE          151.48  \n",
       "DC          136.05  \n",
       "FL          144.18  \n",
       "GA          142.80  \n",
       "HI          120.92  \n",
       "ID           82.75  \n",
       "IL          139.15  \n",
       "IN          108.92  \n",
       "IA          114.47  \n",
       "KS          133.80  \n",
       "KY          137.13  \n",
       "LA          194.78  \n",
       "ME           96.57  \n",
       "MD          192.70  \n",
       "MA          135.63  \n",
       "MI          152.26  \n",
       "MN          133.35  \n",
       "MS          155.77  \n",
       "MO          144.45  \n",
       "MT           85.15  \n",
       "NE          114.82  \n",
       "NV          138.71  \n",
       "NH          120.21  \n",
       "NJ          159.85  \n",
       "NM          120.75  \n",
       "NY          150.01  \n",
       "NC          127.82  \n",
       "ND          109.72  \n",
       "OH          133.52  \n",
       "OK          178.86  \n",
       "OR          104.61  \n",
       "PA          153.86  \n",
       "RI          148.58  \n",
       "SC          116.29  \n",
       "SD           96.87  \n",
       "TN          155.57  \n",
       "TX          156.83  \n",
       "UT          109.48  \n",
       "VT          109.61  \n",
       "VA          153.72  \n",
       "WA          111.62  \n",
       "WV          152.56  \n",
       "WI          106.62  \n",
       "WY          122.04  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.drop(columns='total')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "explanatory = df.iloc[:,1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = df.total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression()"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X=explanatory, y=target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'fit_intercept': True,\n",
       " 'normalize': 'deprecated',\n",
       " 'copy_X': True,\n",
       " 'n_jobs': None,\n",
       " 'positive': False,\n",
       " 'feature_names_in_': array(['speeding', 'alcohol', 'not_distracted', 'no_previous',\n",
       "        'ins_premium', 'ins_losses'], dtype=object),\n",
       " 'n_features_in_': 6,\n",
       " 'coef_': array([-0.02650334,  0.49252176,  0.17453205,  0.71257329, -0.00125105,\n",
       "         0.00643096]),\n",
       " '_residues': 35.75599620119316,\n",
       " 'rank_': 6,\n",
       " 'singular_': array([1265.56295658,  136.88075844,   40.51113699,   14.55337989,\n",
       "          10.99084201,    6.16130337]),\n",
       " 'intercept_': 1.4120634209126202}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.__dict__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network Concepts in Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initializing the `Weights`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - https://keras.io/api/layers/initializers/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to `kernel_initializer` the weights?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "accidents = speeding \\cdot w_1 + alcohol \\cdot w_2 \\ + ... + \\ ins\\_losses \\cdot w_7\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import Sequential, Input\n",
    "from tensorflow.keras.layers import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(51, 7)"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(51, 6)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "explanatory.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(layer=Input(shape=(6,)))\n",
    "model.add(layer=Dense(units=3, kernel_initializer='zeros'))\n",
    "model.add(layer=Dense(units=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Make a Prediction with the Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - Can we make a prediction for for `Washington DC` accidents\n",
    "> - With the already initialized Mathematical Equation?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(columns='total')\n",
    "y = df.total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "AL = X[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>speeding</th>\n",
       "      <th>alcohol</th>\n",
       "      <th>not_distracted</th>\n",
       "      <th>no_previous</th>\n",
       "      <th>ins_premium</th>\n",
       "      <th>ins_losses</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>abbrev</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>AL</th>\n",
       "      <td>7.332</td>\n",
       "      <td>5.64</td>\n",
       "      <td>18.048</td>\n",
       "      <td>15.04</td>\n",
       "      <td>784.55</td>\n",
       "      <td>145.08</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        speeding  alcohol  not_distracted  no_previous  ins_premium  \\\n",
       "abbrev                                                                \n",
       "AL         7.332     5.64          18.048        15.04       784.55   \n",
       "\n",
       "        ins_losses  \n",
       "abbrev              \n",
       "AL          145.08  "
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "AL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  7.332,   5.64 ,  18.048,  15.04 , 784.55 , 145.08 ]])"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "AL.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'abbrev' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/24/tg28vxls25l9mjvqrnh0plc80000gn/T/ipykernel_4945/4121616145.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mabbrev\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'abbrev' is not defined"
     ]
    }
   ],
   "source": [
    "abbrev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 18 calls to <function Model.make_predict_function.<locals>.predict_function at 0x164394670> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-02 11:12:45.067803: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.]], dtype=float32)"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(x=AL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observe the numbers for the `weights`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.]], dtype=float32),\n",
       " array([0., 0., 0.], dtype=float32),\n",
       " array([[-0.38115656],\n",
       "        [ 0.50732684],\n",
       "        [-0.04275787]], dtype=float32),\n",
       " array([0.], dtype=float32)]"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.get_weights()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predictions vs Reality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 1. Calculate the Predicted Accidents and\n",
    "> 2. Compare it with the Real Total Accidents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfsel = df[['total']].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-02 11:28:47.502424: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    }
   ],
   "source": [
    "dfsel['pred_init_0'] = model.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>total</th>\n",
       "      <th>pred_init_0</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>abbrev</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>AL</th>\n",
       "      <td>18.8</td>\n",
       "      <td>-22.534512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AK</th>\n",
       "      <td>18.1</td>\n",
       "      <td>-4.899552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AZ</th>\n",
       "      <td>18.6</td>\n",
       "      <td>-1.247694</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AR</th>\n",
       "      <td>22.4</td>\n",
       "      <td>-15.281178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CA</th>\n",
       "      <td>12.0</td>\n",
       "      <td>-23.931698</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        total  pred_init_0\n",
       "abbrev                    \n",
       "AL       18.8   -22.534512\n",
       "AK       18.1    -4.899552\n",
       "AZ       18.6    -1.247694\n",
       "AR       22.4   -15.281178\n",
       "CA       12.0   -23.931698"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfsel.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `fit()` the `model` and compare again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='mse', metrics=['mse'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 691.6066 - mse: 691.6066\n",
      "Epoch 2/20\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 241.7692 - mse: 241.7692\n",
      "Epoch 3/20\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 149.3006 - mse: 149.3006\n",
      "Epoch 4/20\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 124.2690 - mse: 124.2690\n",
      "Epoch 5/20\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 116.2292 - mse: 116.2292\n",
      "Epoch 6/20\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 112.8786 - mse: 112.8786\n",
      "Epoch 7/20\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 113.0891 - mse: 113.0891\n",
      "Epoch 8/20\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 111.3688 - mse: 111.3688\n",
      "Epoch 9/20\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 113.6378 - mse: 113.6378\n",
      "Epoch 10/20\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 110.6527 - mse: 110.6527\n",
      "Epoch 11/20\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 108.6256 - mse: 108.6256\n",
      "Epoch 12/20\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 109.5691 - mse: 109.5691\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-02 11:28:48.128513: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/20\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 108.1894 - mse: 108.1894\n",
      "Epoch 14/20\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 107.2449 - mse: 107.2449\n",
      "Epoch 15/20\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 105.5857 - mse: 105.5857\n",
      "Epoch 16/20\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 104.4509 - mse: 104.4509\n",
      "Epoch 17/20\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 103.3122 - mse: 103.3122\n",
      "Epoch 18/20\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 103.9633 - mse: 103.9633\n",
      "Epoch 19/20\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 101.5881 - mse: 101.5881\n",
      "Epoch 20/20\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 111.2147 - mse: 111.2147\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x16eb03f40>"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X, y, epochs=20, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Observe the numbers for the `weights`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[-0.23945881, -0.08036486,  0.17776586],\n",
       "        [ 0.18573761,  0.36490265,  0.08089519],\n",
       "        [-0.0224063 ,  0.48527113,  0.26362285],\n",
       "        [ 0.34931043, -0.42759106, -0.65602726],\n",
       "        [-0.6541908 ,  0.12972602, -0.09945652],\n",
       "        [ 0.04073358,  0.24975382,  0.10764756]], dtype=float32),\n",
       " array([-0.02036335, -0.01919998, -0.01933784], dtype=float32),\n",
       " array([[-0.05544065],\n",
       "        [-0.71086824],\n",
       "        [-1.1826289 ]], dtype=float32),\n",
       " array([0.01929409], dtype=float32)]"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.get_weights()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### Predictions vs Reality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 1. Calculate the Predicted Accidents and\n",
    "> 2. Compare it with the Real Total Accidents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-02 11:28:48.899221: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>total</th>\n",
       "      <th>pred_zeros_after_fit</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>abbrev</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>AL</th>\n",
       "      <td>18.8</td>\n",
       "      <td>4.918370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AK</th>\n",
       "      <td>18.1</td>\n",
       "      <td>28.568323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AZ</th>\n",
       "      <td>18.6</td>\n",
       "      <td>27.519278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AR</th>\n",
       "      <td>22.4</td>\n",
       "      <td>13.422966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CA</th>\n",
       "      <td>12.0</td>\n",
       "      <td>5.715553</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        total  pred_zeros_after_fit\n",
       "abbrev                             \n",
       "AL       18.8              4.918370\n",
       "AK       18.1             28.568323\n",
       "AZ       18.6             27.519278\n",
       "AR       22.4             13.422966\n",
       "CA       12.0              5.715553"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfsel = df[['total']].copy()\n",
    "dfsel['pred_zeros_after_fit'] = y_pred\n",
    "dfsel.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "103.45624830057224"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mse = ((dfsel.total - dfsel.pred_zeros_after_fit)**2).mean()\n",
    "mse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to `kernel_initializer` the weights to 1?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(layer=Input(shape=(6,)))\n",
    "model.add(layer=Dense(units=3, kernel_initializer='ones'))\n",
    "model.add(layer=Dense(units=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Make a Prediction with the Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - Can we make a prediction for for `Washington DC` accidents\n",
    "> - With the already initialized Mathematical Equation?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(columns='total')\n",
    "y = df.total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "AL = X[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>speeding</th>\n",
       "      <th>alcohol</th>\n",
       "      <th>not_distracted</th>\n",
       "      <th>no_previous</th>\n",
       "      <th>ins_premium</th>\n",
       "      <th>ins_losses</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>abbrev</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>AL</th>\n",
       "      <td>7.332</td>\n",
       "      <td>5.64</td>\n",
       "      <td>18.048</td>\n",
       "      <td>15.04</td>\n",
       "      <td>784.55</td>\n",
       "      <td>145.08</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        speeding  alcohol  not_distracted  no_previous  ins_premium  \\\n",
       "abbrev                                                                \n",
       "AL         7.332     5.64          18.048        15.04       784.55   \n",
       "\n",
       "        ins_losses  \n",
       "abbrev              \n",
       "AL          145.08  "
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "AL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  7.332,   5.64 ,  18.048,  15.04 , 784.55 , 145.08 ]])"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "AL.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-02 11:28:51.065248: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[-248.40222]], dtype=float32)"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(x=AL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observe the numbers for the `weights`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[1., 1., 1.],\n",
       "        [1., 1., 1.],\n",
       "        [1., 1., 1.],\n",
       "        [1., 1., 1.],\n",
       "        [1., 1., 1.],\n",
       "        [1., 1., 1.]], dtype=float32),\n",
       " array([0., 0., 0.], dtype=float32),\n",
       " array([[-0.4390483],\n",
       "        [-0.7745445],\n",
       "        [ 0.9590014]], dtype=float32),\n",
       " array([0.], dtype=float32)]"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.get_weights()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predictions vs Reality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 1. Calculate the Predicted Accidents and\n",
    "> 2. Compare it with the Real Total Accidents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfsel = df[['total']].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfsel['pred_init_1'] = model.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>total</th>\n",
       "      <th>pred_init_1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>abbrev</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>AL</th>\n",
       "      <td>18.8</td>\n",
       "      <td>-248.402222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AK</th>\n",
       "      <td>18.1</td>\n",
       "      <td>-313.824585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AZ</th>\n",
       "      <td>18.6</td>\n",
       "      <td>-268.598419</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AR</th>\n",
       "      <td>22.4</td>\n",
       "      <td>-260.172485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CA</th>\n",
       "      <td>12.0</td>\n",
       "      <td>-273.227386</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        total  pred_init_1\n",
       "abbrev                    \n",
       "AL       18.8  -248.402222\n",
       "AK       18.1  -313.824585\n",
       "AZ       18.6  -268.598419\n",
       "AR       22.4  -260.172485\n",
       "CA       12.0  -273.227386"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfsel.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `fit()` the `model` and compare again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='mse', metrics=['mse'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 79934.5391 - mse: 79934.5391\n",
      "Epoch 2/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 65447.7578 - mse: 65447.7578\n",
      "Epoch 3/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 56750.2383 - mse: 56750.2383\n",
      "Epoch 4/500\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 49876.5938 - mse: 49876.5938\n",
      "Epoch 5/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 44121.2422 - mse: 44121.2422\n",
      "Epoch 6/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 39142.9375 - mse: 39142.9375\n",
      "Epoch 7/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 34740.3711 - mse: 34740.3711\n",
      "Epoch 8/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 30842.8242 - mse: 30842.8242\n",
      "Epoch 9/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 27350.8516 - mse: 27350.8516\n",
      "Epoch 10/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 24151.9902 - mse: 24151.9902\n",
      "Epoch 11/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 21303.7051 - mse: 21303.7051\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-02 11:28:53.155884: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 18751.1602 - mse: 18751.1602\n",
      "Epoch 13/500\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 16414.3945 - mse: 16414.3945\n",
      "Epoch 14/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 14251.6523 - mse: 14251.6523\n",
      "Epoch 15/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 12324.9707 - mse: 12324.9707\n",
      "Epoch 16/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 10594.6924 - mse: 10594.6924\n",
      "Epoch 17/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 9016.8799 - mse: 9016.8799\n",
      "Epoch 18/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 7585.9058 - mse: 7585.9058\n",
      "Epoch 19/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 6333.7998 - mse: 6333.7998\n",
      "Epoch 20/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 5231.7080 - mse: 5231.7080\n",
      "Epoch 21/500\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 4267.6646 - mse: 4267.6646\n",
      "Epoch 22/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 3445.0032 - mse: 3445.0032\n",
      "Epoch 23/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 2741.4963 - mse: 2741.4963\n",
      "Epoch 24/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 2140.3364 - mse: 2140.3364\n",
      "Epoch 25/500\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 1634.7288 - mse: 1634.7288\n",
      "Epoch 26/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 1223.5415 - mse: 1223.5415\n",
      "Epoch 27/500\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 894.7415 - mse: 894.7415\n",
      "Epoch 28/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 640.3676 - mse: 640.3676\n",
      "Epoch 29/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 444.2414 - mse: 444.2414\n",
      "Epoch 30/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 298.5901 - mse: 298.5901\n",
      "Epoch 31/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 199.2655 - mse: 199.2655\n",
      "Epoch 32/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 131.5880 - mse: 131.5880\n",
      "Epoch 33/500\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 86.1451 - mse: 86.1451\n",
      "Epoch 34/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 57.6906 - mse: 57.6906\n",
      "Epoch 35/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 41.3925 - mse: 41.3925\n",
      "Epoch 36/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 33.7967 - mse: 33.7967\n",
      "Epoch 37/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 29.7898 - mse: 29.7898\n",
      "Epoch 38/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 27.9924 - mse: 27.9924\n",
      "Epoch 39/500\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 27.3170 - mse: 27.3170\n",
      "Epoch 40/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 27.1269 - mse: 27.1269\n",
      "Epoch 41/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 26.9934 - mse: 26.9934\n",
      "Epoch 42/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 27.0549 - mse: 27.0549\n",
      "Epoch 43/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 26.9637 - mse: 26.9637\n",
      "Epoch 44/500\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 26.9615 - mse: 26.9615\n",
      "Epoch 45/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 27.0593 - mse: 27.0593\n",
      "Epoch 46/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 26.8188 - mse: 26.8188\n",
      "Epoch 47/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 27.3410 - mse: 27.3410\n",
      "Epoch 48/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 27.7780 - mse: 27.7780\n",
      "Epoch 49/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 26.7002 - mse: 26.7002\n",
      "Epoch 50/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 26.2943 - mse: 26.2943\n",
      "Epoch 51/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 26.4112 - mse: 26.4112\n",
      "Epoch 52/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 26.3616 - mse: 26.3616\n",
      "Epoch 53/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 25.5519 - mse: 25.5519\n",
      "Epoch 54/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 27.1028 - mse: 27.1028\n",
      "Epoch 55/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 30.6873 - mse: 30.6873\n",
      "Epoch 56/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 26.9577 - mse: 26.9577\n",
      "Epoch 57/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 25.6963 - mse: 25.6963\n",
      "Epoch 58/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 27.7466 - mse: 27.7466\n",
      "Epoch 59/500\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 25.1455 - mse: 25.1455\n",
      "Epoch 60/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 27.3998 - mse: 27.3998\n",
      "Epoch 61/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 51.6861 - mse: 51.6861\n",
      "Epoch 62/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 40.9329 - mse: 40.9329\n",
      "Epoch 63/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 23.0990 - mse: 23.0990\n",
      "Epoch 64/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 24.6042 - mse: 24.6042\n",
      "Epoch 65/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 22.5668 - mse: 22.5668\n",
      "Epoch 66/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 22.1051 - mse: 22.1051\n",
      "Epoch 67/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 26.3183 - mse: 26.3183\n",
      "Epoch 68/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 41.4213 - mse: 41.4213\n",
      "Epoch 69/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 32.0228 - mse: 32.0228\n",
      "Epoch 70/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 21.2921 - mse: 21.2921\n",
      "Epoch 71/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 22.9755 - mse: 22.9755\n",
      "Epoch 72/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 39.3376 - mse: 39.3376\n",
      "Epoch 73/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 37.4338 - mse: 37.4338\n",
      "Epoch 74/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 21.0590 - mse: 21.0590\n",
      "Epoch 75/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 23.6877 - mse: 23.6877\n",
      "Epoch 76/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 20.9168 - mse: 20.9168\n",
      "Epoch 77/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 19.3887 - mse: 19.3887\n",
      "Epoch 78/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 21.1201 - mse: 21.1201\n",
      "Epoch 79/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 30.5441 - mse: 30.5441\n",
      "Epoch 80/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 51.4179 - mse: 51.4179\n",
      "Epoch 81/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 29.5549 - mse: 29.5549\n",
      "Epoch 82/500\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 18.1785 - mse: 18.1785\n",
      "Epoch 83/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 17.9588 - mse: 17.9588\n",
      "Epoch 84/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 20.3506 - mse: 20.3506\n",
      "Epoch 85/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 27.0954 - mse: 27.0954\n",
      "Epoch 86/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 34.6327 - mse: 34.6327\n",
      "Epoch 87/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 23.4746 - mse: 23.4746\n",
      "Epoch 88/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 22.2687 - mse: 22.2687\n",
      "Epoch 89/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 20.2607 - mse: 20.2607\n",
      "Epoch 90/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 18.9038 - mse: 18.9038\n",
      "Epoch 91/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 24.3339 - mse: 24.3339\n",
      "Epoch 92/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 35.0784 - mse: 35.0784\n",
      "Epoch 93/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 40.5321 - mse: 40.5321\n",
      "Epoch 94/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 37.7969 - mse: 37.7969\n",
      "Epoch 95/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 16.9958 - mse: 16.9958\n",
      "Epoch 96/500\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 18.7414 - mse: 18.7414\n",
      "Epoch 97/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 15.6888 - mse: 15.6888\n",
      "Epoch 98/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 15.3068 - mse: 15.3068\n",
      "Epoch 99/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 16.9261 - mse: 16.9261\n",
      "Epoch 100/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 20.0688 - mse: 20.0688\n",
      "Epoch 101/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 26.9097 - mse: 26.9097\n",
      "Epoch 102/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 25.7090 - mse: 25.7090\n",
      "Epoch 103/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 25.6700 - mse: 25.6700\n",
      "Epoch 104/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 19.4791 - mse: 19.4791\n",
      "Epoch 105/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 18.9503 - mse: 18.9503\n",
      "Epoch 106/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 34.5175 - mse: 34.5175\n",
      "Epoch 107/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 33.4476 - mse: 33.4476\n",
      "Epoch 108/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 17.2803 - mse: 17.2803\n",
      "Epoch 109/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 20.0545 - mse: 20.0545\n",
      "Epoch 110/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 13.8795 - mse: 13.8795\n",
      "Epoch 111/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 12.6807 - mse: 12.6807\n",
      "Epoch 112/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 13.1359 - mse: 13.1359\n",
      "Epoch 113/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 19.0088 - mse: 19.0088\n",
      "Epoch 114/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 17.6481 - mse: 17.6481\n",
      "Epoch 115/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 32.4399 - mse: 32.4399\n",
      "Epoch 116/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 17.4240 - mse: 17.4240\n",
      "Epoch 117/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 11.9760 - mse: 11.9760\n",
      "Epoch 118/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 13.2716 - mse: 13.2716\n",
      "Epoch 119/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 11.4932 - mse: 11.4932\n",
      "Epoch 120/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 13.7130 - mse: 13.7130\n",
      "Epoch 121/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 20.5475 - mse: 20.5475\n",
      "Epoch 122/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 47.5730 - mse: 47.5730\n",
      "Epoch 123/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 23.6550 - mse: 23.6550\n",
      "Epoch 124/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 10.8124 - mse: 10.8124\n",
      "Epoch 125/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 10.6860 - mse: 10.6860\n",
      "Epoch 126/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 13.8671 - mse: 13.8671\n",
      "Epoch 127/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 19.3646 - mse: 19.3646\n",
      "Epoch 128/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 25.4310 - mse: 25.4310\n",
      "Epoch 129/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 18.3669 - mse: 18.3669\n",
      "Epoch 130/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 16.3705 - mse: 16.3705\n",
      "Epoch 131/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 18.1509 - mse: 18.1509\n",
      "Epoch 132/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 16.0065 - mse: 16.0065\n",
      "Epoch 133/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 28.9543 - mse: 28.9543\n",
      "Epoch 134/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 20.0502 - mse: 20.0502\n",
      "Epoch 135/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 12.6951 - mse: 12.6951\n",
      "Epoch 136/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 13.6839 - mse: 13.6839\n",
      "Epoch 137/500\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 20.0387 - mse: 20.0387\n",
      "Epoch 138/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 13.6020 - mse: 13.6020\n",
      "Epoch 139/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 10.5686 - mse: 10.5686\n",
      "Epoch 140/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 8.9562 - mse: 8.9562\n",
      "Epoch 141/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 14.7159 - mse: 14.7159\n",
      "Epoch 142/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 28.1774 - mse: 28.1774\n",
      "Epoch 143/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 15.2992 - mse: 15.2992\n",
      "Epoch 144/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 20.7102 - mse: 20.7102\n",
      "Epoch 145/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 16.0313 - mse: 16.0313\n",
      "Epoch 146/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 13.5785 - mse: 13.5785\n",
      "Epoch 147/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 21.4026 - mse: 21.4026\n",
      "Epoch 148/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 22.8557 - mse: 22.8557\n",
      "Epoch 149/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 22.3949 - mse: 22.3949\n",
      "Epoch 150/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 14.0404 - mse: 14.0404\n",
      "Epoch 151/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 9.4920 - mse: 9.4920\n",
      "Epoch 152/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 9.9126 - mse: 9.9126\n",
      "Epoch 153/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 12.1888 - mse: 12.1888\n",
      "Epoch 154/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 30.8583 - mse: 30.8583\n",
      "Epoch 155/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 21.9016 - mse: 21.9016\n",
      "Epoch 156/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 8.5195 - mse: 8.5195\n",
      "Epoch 157/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 11.1606 - mse: 11.1606\n",
      "Epoch 158/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 15.7928 - mse: 15.7928\n",
      "Epoch 159/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 7.8361 - mse: 7.8361\n",
      "Epoch 160/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 8.2404 - mse: 8.2404\n",
      "Epoch 161/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 13.6377 - mse: 13.6377\n",
      "Epoch 162/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 29.1095 - mse: 29.1095\n",
      "Epoch 163/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 14.0624 - mse: 14.0624\n",
      "Epoch 164/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 7.3707 - mse: 7.3707\n",
      "Epoch 165/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 7.7856 - mse: 7.7856\n",
      "Epoch 166/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 9.8234 - mse: 9.8234\n",
      "Epoch 167/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 23.2560 - mse: 23.2560\n",
      "Epoch 168/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 28.9655 - mse: 28.9655\n",
      "Epoch 169/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 17.3882 - mse: 17.3882\n",
      "Epoch 170/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 6.8604 - mse: 6.8604\n",
      "Epoch 171/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 7.8882 - mse: 7.8882\n",
      "Epoch 172/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 12.7912 - mse: 12.7912\n",
      "Epoch 173/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 16.4068 - mse: 16.4068\n",
      "Epoch 174/500\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 21.7748 - mse: 21.7748\n",
      "Epoch 175/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 14.8850 - mse: 14.8850\n",
      "Epoch 176/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 8.2214 - mse: 8.2214\n",
      "Epoch 177/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 6.3370 - mse: 6.3370\n",
      "Epoch 178/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 10.0703 - mse: 10.0703\n",
      "Epoch 179/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 18.1172 - mse: 18.1172\n",
      "Epoch 180/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 19.3662 - mse: 19.3662\n",
      "Epoch 181/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 8.9195 - mse: 8.9195\n",
      "Epoch 182/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 7.5226 - mse: 7.5226\n",
      "Epoch 183/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 8.8787 - mse: 8.8787\n",
      "Epoch 184/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 12.6342 - mse: 12.6342\n",
      "Epoch 185/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 12.7336 - mse: 12.7336\n",
      "Epoch 186/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 18.9630 - mse: 18.9630\n",
      "Epoch 187/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 29.5169 - mse: 29.5169\n",
      "Epoch 188/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 12.4004 - mse: 12.4004\n",
      "Epoch 189/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 9.2065 - mse: 9.2065\n",
      "Epoch 190/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 8.6878 - mse: 8.6878\n",
      "Epoch 191/500\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 7.1296 - mse: 7.1296\n",
      "Epoch 192/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 12.6540 - mse: 12.6540\n",
      "Epoch 193/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 16.0176 - mse: 16.0176\n",
      "Epoch 194/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 15.5611 - mse: 15.5611\n",
      "Epoch 195/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 15.7941 - mse: 15.7941\n",
      "Epoch 196/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 7.6237 - mse: 7.6237\n",
      "Epoch 197/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 5.0061 - mse: 5.0061\n",
      "Epoch 198/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 5.0740 - mse: 5.0740\n",
      "Epoch 199/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 4.9225 - mse: 4.9225\n",
      "Epoch 200/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 5.0026 - mse: 5.0026\n",
      "Epoch 201/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 10.0712 - mse: 10.0712\n",
      "Epoch 202/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 50.7549 - mse: 50.7549\n",
      "Epoch 203/500\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 19.0463 - mse: 19.0463\n",
      "Epoch 204/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 6.1852 - mse: 6.1852\n",
      "Epoch 205/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 4.5782 - mse: 4.5782\n",
      "Epoch 206/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 4.6682 - mse: 4.6682\n",
      "Epoch 207/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 5.7155 - mse: 5.7155\n",
      "Epoch 208/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 11.6468 - mse: 11.6468\n",
      "Epoch 209/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 29.8747 - mse: 29.8747\n",
      "Epoch 210/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 19.1917 - mse: 19.1917\n",
      "Epoch 211/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 7.0082 - mse: 7.0082\n",
      "Epoch 212/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 6.9103 - mse: 6.9103\n",
      "Epoch 213/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 13.4568 - mse: 13.4568\n",
      "Epoch 214/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 19.8171 - mse: 19.8171\n",
      "Epoch 215/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 11.8877 - mse: 11.8877\n",
      "Epoch 216/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 8.6743 - mse: 8.6743\n",
      "Epoch 217/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 5.7611 - mse: 5.7611\n",
      "Epoch 218/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 4.6271 - mse: 4.6271\n",
      "Epoch 219/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 12.2127 - mse: 12.2127\n",
      "Epoch 220/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 22.5064 - mse: 22.5064\n",
      "Epoch 221/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 20.1754 - mse: 20.1754\n",
      "Epoch 222/500\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 4.0325 - mse: 4.0325\n",
      "Epoch 223/500\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 3.9312 - mse: 3.9312\n",
      "Epoch 224/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 5.5007 - mse: 5.5007\n",
      "Epoch 225/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 5.1714 - mse: 5.1714\n",
      "Epoch 226/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 10.9253 - mse: 10.9253\n",
      "Epoch 227/500\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 28.2864 - mse: 28.2864\n",
      "Epoch 228/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 17.7610 - mse: 17.7610\n",
      "Epoch 229/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 5.8413 - mse: 5.8413\n",
      "Epoch 230/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 3.7305 - mse: 3.7305\n",
      "Epoch 231/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 4.2170 - mse: 4.2170\n",
      "Epoch 232/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 3.7270 - mse: 3.7270\n",
      "Epoch 233/500\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 8.3103 - mse: 8.3103\n",
      "Epoch 234/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 26.1269 - mse: 26.1269\n",
      "Epoch 235/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 20.3050 - mse: 20.3050\n",
      "Epoch 236/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 11.5541 - mse: 11.5541\n",
      "Epoch 237/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 5.8987 - mse: 5.8987\n",
      "Epoch 238/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 6.1778 - mse: 6.1778\n",
      "Epoch 239/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 13.3809 - mse: 13.3809\n",
      "Epoch 240/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 14.9635 - mse: 14.9635\n",
      "Epoch 241/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 4.8574 - mse: 4.8574\n",
      "Epoch 242/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 4.6553 - mse: 4.6553\n",
      "Epoch 243/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 4.8900 - mse: 4.8900\n",
      "Epoch 244/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 6.2764 - mse: 6.2764\n",
      "Epoch 245/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 20.8921 - mse: 20.8921\n",
      "Epoch 246/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 20.1721 - mse: 20.1721\n",
      "Epoch 247/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 4.7802 - mse: 4.7802\n",
      "Epoch 248/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 3.2915 - mse: 3.2915\n",
      "Epoch 249/500\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 3.4250 - mse: 3.4250\n",
      "Epoch 250/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 4.4904 - mse: 4.4904\n",
      "Epoch 251/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 13.3978 - mse: 13.3978\n",
      "Epoch 252/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 25.7946 - mse: 25.7946\n",
      "Epoch 253/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 17.7554 - mse: 17.7554\n",
      "Epoch 254/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 13.4339 - mse: 13.4339\n",
      "Epoch 255/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 7.3496 - mse: 7.3496\n",
      "Epoch 256/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 4.5796 - mse: 4.5796\n",
      "Epoch 257/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 3.1043 - mse: 3.1043\n",
      "Epoch 258/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 3.2427 - mse: 3.2427\n",
      "Epoch 259/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 3.0650 - mse: 3.0650\n",
      "Epoch 260/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 3.0817 - mse: 3.0817\n",
      "Epoch 261/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 10.4401 - mse: 10.4401\n",
      "Epoch 262/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 45.5145 - mse: 45.5145\n",
      "Epoch 263/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 15.0879 - mse: 15.0879\n",
      "Epoch 264/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 3.2478 - mse: 3.2478\n",
      "Epoch 265/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 3.9138 - mse: 3.9138\n",
      "Epoch 266/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 2.9562 - mse: 2.9562\n",
      "Epoch 267/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 3.0643 - mse: 3.0643\n",
      "Epoch 268/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 4.0891 - mse: 4.0891\n",
      "Epoch 269/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 15.5415 - mse: 15.5415\n",
      "Epoch 270/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 30.0697 - mse: 30.0697\n",
      "Epoch 271/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 12.7012 - mse: 12.7012\n",
      "Epoch 272/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 5.7208 - mse: 5.7208\n",
      "Epoch 273/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 3.5411 - mse: 3.5411\n",
      "Epoch 274/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 2.7991 - mse: 2.7991\n",
      "Epoch 275/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 3.5808 - mse: 3.5808\n",
      "Epoch 276/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 9.8761 - mse: 9.8761\n",
      "Epoch 277/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 22.6808 - mse: 22.6808\n",
      "Epoch 278/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 16.8234 - mse: 16.8234\n",
      "Epoch 279/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 10.6476 - mse: 10.6476\n",
      "Epoch 280/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 8.8480 - mse: 8.8480\n",
      "Epoch 281/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 6.7275 - mse: 6.7275\n",
      "Epoch 282/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 3.6568 - mse: 3.6568\n",
      "Epoch 283/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 2.6785 - mse: 2.6785\n",
      "Epoch 284/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 3.7256 - mse: 3.7256\n",
      "Epoch 285/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 6.3168 - mse: 6.3168\n",
      "Epoch 286/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 17.1707 - mse: 17.1707\n",
      "Epoch 287/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 31.7202 - mse: 31.7202\n",
      "Epoch 288/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 17.3416 - mse: 17.3416\n",
      "Epoch 289/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 5.1387 - mse: 5.1387\n",
      "Epoch 290/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 4.3129 - mse: 4.3129\n",
      "Epoch 291/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 3.6726 - mse: 3.6726\n",
      "Epoch 292/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 3.0262 - mse: 3.0262\n",
      "Epoch 293/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 5.0875 - mse: 5.0875\n",
      "Epoch 294/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 10.7937 - mse: 10.7937\n",
      "Epoch 295/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 18.6518 - mse: 18.6518\n",
      "Epoch 296/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 15.7834 - mse: 15.7834\n",
      "Epoch 297/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 4.8373 - mse: 4.8373\n",
      "Epoch 298/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 2.5798 - mse: 2.5798\n",
      "Epoch 299/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 2.4770 - mse: 2.4770\n",
      "Epoch 300/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 2.5179 - mse: 2.5179\n",
      "Epoch 301/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 3.7422 - mse: 3.7422\n",
      "Epoch 302/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 12.9079 - mse: 12.9079\n",
      "Epoch 303/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 38.0269 - mse: 38.0269\n",
      "Epoch 304/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 8.6692 - mse: 8.6692\n",
      "Epoch 305/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 5.2344 - mse: 5.2344\n",
      "Epoch 306/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 5.8300 - mse: 5.8300\n",
      "Epoch 307/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 2.6991 - mse: 2.6991\n",
      "Epoch 308/500\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 2.5804 - mse: 2.5804\n",
      "Epoch 309/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 3.7750 - mse: 3.7750\n",
      "Epoch 310/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 10.1767 - mse: 10.1767\n",
      "Epoch 311/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 27.8549 - mse: 27.8549\n",
      "Epoch 312/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 18.6297 - mse: 18.6297\n",
      "Epoch 313/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 8.9912 - mse: 8.9912\n",
      "Epoch 314/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 3.9530 - mse: 3.9530\n",
      "Epoch 315/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 4.8990 - mse: 4.8990\n",
      "Epoch 316/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 3.3608 - mse: 3.3608\n",
      "Epoch 317/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 4.7117 - mse: 4.7117\n",
      "Epoch 318/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 11.5437 - mse: 11.5437\n",
      "Epoch 319/500\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 25.1718 - mse: 25.1718\n",
      "Epoch 320/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 15.3738 - mse: 15.3738\n",
      "Epoch 321/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 2.2604 - mse: 2.2604\n",
      "Epoch 322/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 2.3584 - mse: 2.3584\n",
      "Epoch 323/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 2.4529 - mse: 2.4529\n",
      "Epoch 324/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 4.5098 - mse: 4.5098\n",
      "Epoch 325/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 15.2640 - mse: 15.2640\n",
      "Epoch 326/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 20.7781 - mse: 20.7781\n",
      "Epoch 327/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 12.5210 - mse: 12.5210\n",
      "Epoch 328/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 6.6497 - mse: 6.6497\n",
      "Epoch 329/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 3.5267 - mse: 3.5267\n",
      "Epoch 330/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 2.1372 - mse: 2.1372\n",
      "Epoch 331/500\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 2.2039 - mse: 2.2039\n",
      "Epoch 332/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 2.7098 - mse: 2.7098\n",
      "Epoch 333/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 12.4457 - mse: 12.4457\n",
      "Epoch 334/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 23.6395 - mse: 23.6395\n",
      "Epoch 335/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 22.4834 - mse: 22.4834\n",
      "Epoch 336/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 11.6869 - mse: 11.6869\n",
      "Epoch 337/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 5.2629 - mse: 5.2629\n",
      "Epoch 338/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 3.0260 - mse: 3.0260\n",
      "Epoch 339/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 2.6884 - mse: 2.6884\n",
      "Epoch 340/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 2.6188 - mse: 2.6188\n",
      "Epoch 341/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 8.7214 - mse: 8.7214\n",
      "Epoch 342/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 19.0187 - mse: 19.0187\n",
      "Epoch 343/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 19.3524 - mse: 19.3524\n",
      "Epoch 344/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 11.3847 - mse: 11.3847\n",
      "Epoch 345/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 5.2424 - mse: 5.2424\n",
      "Epoch 346/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 4.8914 - mse: 4.8914\n",
      "Epoch 347/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 9.2024 - mse: 9.2024\n",
      "Epoch 348/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 21.8880 - mse: 21.8880\n",
      "Epoch 349/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 13.2628 - mse: 13.2628\n",
      "Epoch 350/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 3.0055 - mse: 3.0055\n",
      "Epoch 351/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 2.0172 - mse: 2.0172\n",
      "Epoch 352/500\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 1.9917 - mse: 1.9917\n",
      "Epoch 353/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 3.9527 - mse: 3.9527\n",
      "Epoch 354/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 21.3824 - mse: 21.3824\n",
      "Epoch 355/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 32.9181 - mse: 32.9181\n",
      "Epoch 356/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 6.6154 - mse: 6.6154\n",
      "Epoch 357/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 4.7506 - mse: 4.7506\n",
      "Epoch 358/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 4.6441 - mse: 4.6441\n",
      "Epoch 359/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 2.2673 - mse: 2.2673\n",
      "Epoch 360/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 1.9457 - mse: 1.9457\n",
      "Epoch 361/500\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 2.0237 - mse: 2.0237\n",
      "Epoch 362/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 1.9044 - mse: 1.9044\n",
      "Epoch 363/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 4.1665 - mse: 4.1665\n",
      "Epoch 364/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 28.4194 - mse: 28.4194\n",
      "Epoch 365/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 28.4321 - mse: 28.4321\n",
      "Epoch 366/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 6.4997 - mse: 6.4997\n",
      "Epoch 367/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 2.6307 - mse: 2.6307\n",
      "Epoch 368/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 1.9029 - mse: 1.9029\n",
      "Epoch 369/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 2.1055 - mse: 2.1055\n",
      "Epoch 370/500\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 3.3590 - mse: 3.3590\n",
      "Epoch 371/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 6.1004 - mse: 6.1004\n",
      "Epoch 372/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 23.1929 - mse: 23.1929\n",
      "Epoch 373/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 20.8064 - mse: 20.8064\n",
      "Epoch 374/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 8.2923 - mse: 8.2923\n",
      "Epoch 375/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 3.8789 - mse: 3.8789\n",
      "Epoch 376/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 2.4007 - mse: 2.4007\n",
      "Epoch 377/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 3.0520 - mse: 3.0520\n",
      "Epoch 378/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 5.3591 - mse: 5.3591\n",
      "Epoch 379/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 14.5676 - mse: 14.5676\n",
      "Epoch 380/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 19.9938 - mse: 19.9938\n",
      "Epoch 381/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 12.5201 - mse: 12.5201\n",
      "Epoch 382/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 8.3816 - mse: 8.3816\n",
      "Epoch 383/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 3.0249 - mse: 3.0249\n",
      "Epoch 384/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 2.1892 - mse: 2.1892\n",
      "Epoch 385/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 1.8267 - mse: 1.8267\n",
      "Epoch 386/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 2.2054 - mse: 2.2054\n",
      "Epoch 387/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 4.4054 - mse: 4.4054\n",
      "Epoch 388/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 12.8878 - mse: 12.8878\n",
      "Epoch 389/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 27.2688 - mse: 27.2688\n",
      "Epoch 390/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 18.4260 - mse: 18.4260\n",
      "Epoch 391/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 2.8936 - mse: 2.8936\n",
      "Epoch 392/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 1.8180 - mse: 1.8180\n",
      "Epoch 393/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 2.0814 - mse: 2.0814\n",
      "Epoch 394/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 1.9587 - mse: 1.9587\n",
      "Epoch 395/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 3.2824 - mse: 3.2824\n",
      "Epoch 396/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 19.5804 - mse: 19.5804\n",
      "Epoch 397/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 21.2822 - mse: 21.2822\n",
      "Epoch 398/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 10.0948 - mse: 10.0948\n",
      "Epoch 399/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 7.3461 - mse: 7.3461\n",
      "Epoch 400/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 4.6712 - mse: 4.6712\n",
      "Epoch 401/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 5.5246 - mse: 5.5246\n",
      "Epoch 402/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 9.5345 - mse: 9.5345\n",
      "Epoch 403/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 7.8187 - mse: 7.8187\n",
      "Epoch 404/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 7.9612 - mse: 7.9612\n",
      "Epoch 405/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 7.8121 - mse: 7.8121\n",
      "Epoch 406/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 11.2407 - mse: 11.2407\n",
      "Epoch 407/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 19.2099 - mse: 19.2099\n",
      "Epoch 408/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 14.8336 - mse: 14.8336\n",
      "Epoch 409/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 6.1520 - mse: 6.1520\n",
      "Epoch 410/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 9.6918 - mse: 9.6918\n",
      "Epoch 411/500\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 7.7067 - mse: 7.7067\n",
      "Epoch 412/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 2.6054 - mse: 2.6054\n",
      "Epoch 413/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 2.3693 - mse: 2.3693\n",
      "Epoch 414/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 7.3817 - mse: 7.3817\n",
      "Epoch 415/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 17.0444 - mse: 17.0444\n",
      "Epoch 416/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 12.4180 - mse: 12.4180\n",
      "Epoch 417/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 8.0189 - mse: 8.0189\n",
      "Epoch 418/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 7.5717 - mse: 7.5717\n",
      "Epoch 419/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 5.3032 - mse: 5.3032\n",
      "Epoch 420/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 6.6496 - mse: 6.6496\n",
      "Epoch 421/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 9.4040 - mse: 9.4040\n",
      "Epoch 422/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 15.6117 - mse: 15.6117\n",
      "Epoch 423/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 12.1420 - mse: 12.1420\n",
      "Epoch 424/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 7.8900 - mse: 7.8900\n",
      "Epoch 425/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 4.9700 - mse: 4.9700\n",
      "Epoch 426/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 2.4974 - mse: 2.4974\n",
      "Epoch 427/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 2.2941 - mse: 2.2941\n",
      "Epoch 428/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 6.7686 - mse: 6.7686\n",
      "Epoch 429/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 17.7106 - mse: 17.7106\n",
      "Epoch 430/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 22.5153 - mse: 22.5153\n",
      "Epoch 431/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 5.8996 - mse: 5.8996\n",
      "Epoch 432/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 1.7803 - mse: 1.7803\n",
      "Epoch 433/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 3.7516 - mse: 3.7516\n",
      "Epoch 434/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 5.8197 - mse: 5.8197\n",
      "Epoch 435/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 13.1933 - mse: 13.1933\n",
      "Epoch 436/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 15.9346 - mse: 15.9346\n",
      "Epoch 437/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 9.9746 - mse: 9.9746\n",
      "Epoch 438/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 7.0578 - mse: 7.0578\n",
      "Epoch 439/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 9.7158 - mse: 9.7158\n",
      "Epoch 440/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 6.4612 - mse: 6.4612\n",
      "Epoch 441/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 9.8204 - mse: 9.8204\n",
      "Epoch 442/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 6.2701 - mse: 6.2701\n",
      "Epoch 443/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 11.0596 - mse: 11.0596\n",
      "Epoch 444/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 15.0453 - mse: 15.0453\n",
      "Epoch 445/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 13.1868 - mse: 13.1868\n",
      "Epoch 446/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 8.2880 - mse: 8.2880\n",
      "Epoch 447/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 4.0725 - mse: 4.0725\n",
      "Epoch 448/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 2.6709 - mse: 2.6709\n",
      "Epoch 449/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 3.3991 - mse: 3.3991\n",
      "Epoch 450/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 15.0667 - mse: 15.0667\n",
      "Epoch 451/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 19.2874 - mse: 19.2874\n",
      "Epoch 452/500\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 9.4070 - mse: 9.4070\n",
      "Epoch 453/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 5.4615 - mse: 5.4615\n",
      "Epoch 454/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 2.2306 - mse: 2.2306\n",
      "Epoch 455/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 2.3517 - mse: 2.3517\n",
      "Epoch 456/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 1.9045 - mse: 1.9045\n",
      "Epoch 457/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 1.8088 - mse: 1.8088\n",
      "Epoch 458/500\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 3.5334 - mse: 3.5334\n",
      "Epoch 459/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 26.7744 - mse: 26.7744\n",
      "Epoch 460/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 27.7261 - mse: 27.7261\n",
      "Epoch 461/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 4.3961 - mse: 4.3961\n",
      "Epoch 462/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 2.9170 - mse: 2.9170\n",
      "Epoch 463/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 5.1662 - mse: 5.1662\n",
      "Epoch 464/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 5.9054 - mse: 5.9054\n",
      "Epoch 465/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 3.2866 - mse: 3.2866\n",
      "Epoch 466/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 2.1472 - mse: 2.1472\n",
      "Epoch 467/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 1.6499 - mse: 1.6499\n",
      "Epoch 468/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 2.3823 - mse: 2.3823\n",
      "Epoch 469/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 17.3106 - mse: 17.3106\n",
      "Epoch 470/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 40.7005 - mse: 40.7005\n",
      "Epoch 471/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 7.7141 - mse: 7.7141\n",
      "Epoch 472/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 1.8886 - mse: 1.8886\n",
      "Epoch 473/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 1.9709 - mse: 1.9709\n",
      "Epoch 474/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 3.2371 - mse: 3.2371\n",
      "Epoch 475/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 4.3506 - mse: 4.3506\n",
      "Epoch 476/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 8.5978 - mse: 8.5978\n",
      "Epoch 477/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 20.2635 - mse: 20.2635\n",
      "Epoch 478/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 12.6608 - mse: 12.6608\n",
      "Epoch 479/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 4.7869 - mse: 4.7869\n",
      "Epoch 480/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 2.8505 - mse: 2.8505\n",
      "Epoch 481/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 3.2643 - mse: 3.2643\n",
      "Epoch 482/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 5.3465 - mse: 5.3465\n",
      "Epoch 483/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 12.4409 - mse: 12.4409\n",
      "Epoch 484/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 21.2011 - mse: 21.2011\n",
      "Epoch 485/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 9.7520 - mse: 9.7520\n",
      "Epoch 486/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 3.7376 - mse: 3.7376\n",
      "Epoch 487/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 3.5181 - mse: 3.5181\n",
      "Epoch 488/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 2.6132 - mse: 2.6132\n",
      "Epoch 489/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 2.7073 - mse: 2.7073\n",
      "Epoch 490/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 3.5094 - mse: 3.5094\n",
      "Epoch 491/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 14.3242 - mse: 14.3242\n",
      "Epoch 492/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 31.3174 - mse: 31.3174\n",
      "Epoch 493/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 16.0512 - mse: 16.0512\n",
      "Epoch 494/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 3.9808 - mse: 3.9808\n",
      "Epoch 495/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 2.5710 - mse: 2.5710\n",
      "Epoch 496/500\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 1.5883 - mse: 1.5883\n",
      "Epoch 497/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 1.6469 - mse: 1.6469\n",
      "Epoch 498/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 1.6454 - mse: 1.6454\n",
      "Epoch 499/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 4.1233 - mse: 4.1233\n",
      "Epoch 500/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 24.8661 - mse: 24.8661\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x295aa0400>"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X, y, epochs=500, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Observe the numbers for the `weights`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[0.8258562 , 0.8255719 , 1.1750928 ],\n",
       "        [0.7655462 , 0.7652197 , 1.235535  ],\n",
       "        [0.79672956, 0.79642075, 1.2042937 ],\n",
       "        [0.77296555, 0.7726575 , 1.228058  ],\n",
       "        [0.9560893 , 0.9560746 , 1.0440463 ],\n",
       "        [0.94230795, 0.94224495, 1.057977  ]], dtype=float32),\n",
       " array([-0.1104157 , -0.11054494,  0.11090087], dtype=float32),\n",
       " array([[-0.38780054],\n",
       "        [-0.7233023 ],\n",
       "        [ 1.0113761 ]], dtype=float32),\n",
       " array([0.11073126], dtype=float32)]"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.get_weights()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### Predictions vs Reality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 1. Calculate the Predicted Accidents and\n",
    "> 2. Compare it with the Real Total Accidents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-02 11:28:59.711542: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>total</th>\n",
       "      <th>pred_init_1</th>\n",
       "      <th>pred_1_after_fit</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>abbrev</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>AL</th>\n",
       "      <td>18.8</td>\n",
       "      <td>-248.402222</td>\n",
       "      <td>14.700633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AK</th>\n",
       "      <td>18.1</td>\n",
       "      <td>-313.824585</td>\n",
       "      <td>12.478535</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AZ</th>\n",
       "      <td>18.6</td>\n",
       "      <td>-268.598419</td>\n",
       "      <td>13.044076</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AR</th>\n",
       "      <td>22.4</td>\n",
       "      <td>-260.172485</td>\n",
       "      <td>16.938688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CA</th>\n",
       "      <td>12.0</td>\n",
       "      <td>-273.227386</td>\n",
       "      <td>8.771456</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        total  pred_init_1  pred_1_after_fit\n",
       "abbrev                                      \n",
       "AL       18.8  -248.402222         14.700633\n",
       "AK       18.1  -313.824585         12.478535\n",
       "AZ       18.6  -268.598419         13.044076\n",
       "AR       22.4  -260.172485         16.938688\n",
       "CA       12.0  -273.227386          8.771456"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfsel['pred_1_after_fit'] = y_pred\n",
    "dfsel.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24.950977330421093"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mse = ((dfsel.total - dfsel.pred_1_after_fit)**2).mean()\n",
    "mse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to `kernel_initializer` the weights to `glorot_uniform` (default)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(layer=Input(shape=(6,)))\n",
    "model.add(layer=Dense(units=3, kernel_initializer='glorot_uniform'))\n",
    "model.add(layer=Dense(units=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Make a Prediction with the Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - Can we make a prediction for for `Washington DC` accidents\n",
    "> - With the already initialized Mathematical Equation?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(columns='total')\n",
    "y = df.total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "AL = X[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>speeding</th>\n",
       "      <th>alcohol</th>\n",
       "      <th>not_distracted</th>\n",
       "      <th>no_previous</th>\n",
       "      <th>ins_premium</th>\n",
       "      <th>ins_losses</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>abbrev</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>AL</th>\n",
       "      <td>7.332</td>\n",
       "      <td>5.64</td>\n",
       "      <td>18.048</td>\n",
       "      <td>15.04</td>\n",
       "      <td>784.55</td>\n",
       "      <td>145.08</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        speeding  alcohol  not_distracted  no_previous  ins_premium  \\\n",
       "abbrev                                                                \n",
       "AL         7.332     5.64          18.048        15.04       784.55   \n",
       "\n",
       "        ins_losses  \n",
       "abbrev              \n",
       "AL          145.08  "
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "AL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  7.332,   5.64 ,  18.048,  15.04 , 784.55 , 145.08 ]])"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "AL.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-02 11:28:59.806758: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[-257.93704]], dtype=float32)"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(x=AL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observe the numbers for the `weights`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[ 0.3402233 ,  0.07687533,  0.15121126],\n",
       "        [ 0.27671444, -0.79064023, -0.07849264],\n",
       "        [ 0.26316738, -0.68986434, -0.5974864 ],\n",
       "        [ 0.11316693, -0.5042963 ,  0.64271986],\n",
       "        [ 0.00982642, -0.55679655, -0.5014933 ],\n",
       "        [-0.40654555, -0.33385673, -0.5001458 ]], dtype=float32),\n",
       " array([0., 0., 0.], dtype=float32),\n",
       " array([[ 0.91148007],\n",
       "        [-0.62716556],\n",
       "        [ 1.1579458 ]], dtype=float32),\n",
       " array([0.], dtype=float32)]"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.get_weights()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predictions vs Reality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 1. Calculate the Predicted Accidents and\n",
    "> 2. Compare it with the Real Total Accidents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfsel = df[['total']].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfsel['pred_init_1'] = model.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>total</th>\n",
       "      <th>pred_init_1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>abbrev</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>AL</th>\n",
       "      <td>18.8</td>\n",
       "      <td>-257.937042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AK</th>\n",
       "      <td>18.1</td>\n",
       "      <td>-307.893494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AZ</th>\n",
       "      <td>18.6</td>\n",
       "      <td>-255.119736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AR</th>\n",
       "      <td>22.4</td>\n",
       "      <td>-259.586212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CA</th>\n",
       "      <td>12.0</td>\n",
       "      <td>-301.841125</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        total  pred_init_1\n",
       "abbrev                    \n",
       "AL       18.8  -257.937042\n",
       "AK       18.1  -307.893494\n",
       "AZ       18.6  -255.119736\n",
       "AR       22.4  -259.586212\n",
       "CA       12.0  -301.841125"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfsel.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `fit()` the `model` and compare again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='mse', metrics=['mse'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 84722.8281 - mse: 84722.8281\n",
      "Epoch 2/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 73576.1172 - mse: 73576.1172\n",
      "Epoch 3/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 66624.0156 - mse: 66624.0156\n",
      "Epoch 4/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 61058.3047 - mse: 61058.3047\n",
      "Epoch 5/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 56244.8438 - mse: 56244.8438\n",
      "Epoch 6/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 51967.0352 - mse: 51967.0352\n",
      "Epoch 7/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 48141.5352 - mse: 48141.5352\n",
      "Epoch 8/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 44659.2695 - mse: 44659.2695\n",
      "Epoch 9/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 41418.8984 - mse: 41418.8984\n",
      "Epoch 10/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 38412.9766 - mse: 38412.9766\n",
      "Epoch 11/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 35569.6992 - mse: 35569.6992\n",
      "Epoch 12/500\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 33280.6641 - mse: 33280.6641"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-02 11:29:01.758049: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 6ms/step - loss: 32843.1289 - mse: 32843.1289\n",
      "Epoch 13/500\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 30310.1855 - mse: 30310.1855\n",
      "Epoch 14/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 27925.7070 - mse: 27925.7070\n",
      "Epoch 15/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 25673.5938 - mse: 25673.5938\n",
      "Epoch 16/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 23553.9707 - mse: 23553.9707\n",
      "Epoch 17/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 21561.5723 - mse: 21561.5723\n",
      "Epoch 18/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 19663.9590 - mse: 19663.9590\n",
      "Epoch 19/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 17880.5820 - mse: 17880.5820\n",
      "Epoch 20/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 16187.2256 - mse: 16187.2256\n",
      "Epoch 21/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 14587.0742 - mse: 14587.0742\n",
      "Epoch 22/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 13103.4805 - mse: 13103.4805\n",
      "Epoch 23/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 11715.6914 - mse: 11715.6914\n",
      "Epoch 24/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 10405.9180 - mse: 10405.9180\n",
      "Epoch 25/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 9213.9131 - mse: 9213.9131\n",
      "Epoch 26/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 8110.5376 - mse: 8110.5376\n",
      "Epoch 27/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 7083.2280 - mse: 7083.2280\n",
      "Epoch 28/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 6138.6729 - mse: 6138.6729\n",
      "Epoch 29/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 5280.5747 - mse: 5280.5747\n",
      "Epoch 30/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 4508.3267 - mse: 4508.3267\n",
      "Epoch 31/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 3818.9878 - mse: 3818.9878\n",
      "Epoch 32/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 3196.8281 - mse: 3196.8281\n",
      "Epoch 33/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 2636.7661 - mse: 2636.7661\n",
      "Epoch 34/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 2145.8276 - mse: 2145.8276\n",
      "Epoch 35/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 1736.8524 - mse: 1736.8524\n",
      "Epoch 36/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 1387.5344 - mse: 1387.5344\n",
      "Epoch 37/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 1090.3979 - mse: 1090.3979\n",
      "Epoch 38/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 844.2575 - mse: 844.2575\n",
      "Epoch 39/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 648.5540 - mse: 648.5540\n",
      "Epoch 40/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 495.7996 - mse: 495.7996\n",
      "Epoch 41/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 377.6850 - mse: 377.6850\n",
      "Epoch 42/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 285.2530 - mse: 285.2530\n",
      "Epoch 43/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 222.1207 - mse: 222.1207\n",
      "Epoch 44/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 178.4536 - mse: 178.4536\n",
      "Epoch 45/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 148.9454 - mse: 148.9454\n",
      "Epoch 46/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 128.0709 - mse: 128.0709\n",
      "Epoch 47/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 117.1776 - mse: 117.1776\n",
      "Epoch 48/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 111.4624 - mse: 111.4624\n",
      "Epoch 49/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 108.4552 - mse: 108.4552\n",
      "Epoch 50/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 107.0247 - mse: 107.0247\n",
      "Epoch 51/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 106.8258 - mse: 106.8258\n",
      "Epoch 52/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 106.3279 - mse: 106.3279\n",
      "Epoch 53/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 106.5886 - mse: 106.5886\n",
      "Epoch 54/500\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 105.5532 - mse: 105.5532\n",
      "Epoch 55/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 105.0170 - mse: 105.0170\n",
      "Epoch 56/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 107.4008 - mse: 107.4008\n",
      "Epoch 57/500\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 109.5113 - mse: 109.5113\n",
      "Epoch 58/500\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 106.4870 - mse: 106.4870\n",
      "Epoch 59/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 104.0689 - mse: 104.0689\n",
      "Epoch 60/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 103.3698 - mse: 103.3698\n",
      "Epoch 61/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 106.3907 - mse: 106.3907\n",
      "Epoch 62/500\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 103.5296 - mse: 103.5296\n",
      "Epoch 63/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 108.2224 - mse: 108.2224\n",
      "Epoch 64/500\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 105.6248 - mse: 105.6248\n",
      "Epoch 65/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 100.3941 - mse: 100.3941\n",
      "Epoch 66/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 100.2322 - mse: 100.2322\n",
      "Epoch 67/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 98.4937 - mse: 98.4937\n",
      "Epoch 68/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 101.6312 - mse: 101.6312\n",
      "Epoch 69/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 103.5376 - mse: 103.5376\n",
      "Epoch 70/500\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 108.4002 - mse: 108.4002\n",
      "Epoch 71/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 100.2767 - mse: 100.2767\n",
      "Epoch 72/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 94.9399 - mse: 94.9399\n",
      "Epoch 73/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 95.2227 - mse: 95.2227\n",
      "Epoch 74/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 96.7269 - mse: 96.7269\n",
      "Epoch 75/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 104.9887 - mse: 104.9887\n",
      "Epoch 76/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 95.0748 - mse: 95.0748\n",
      "Epoch 77/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 92.1962 - mse: 92.1962\n",
      "Epoch 78/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 100.4992 - mse: 100.4992\n",
      "Epoch 79/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 98.9742 - mse: 98.9742\n",
      "Epoch 80/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 97.8050 - mse: 97.8050\n",
      "Epoch 81/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 87.4330 - mse: 87.4330\n",
      "Epoch 82/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 87.1768 - mse: 87.1768\n",
      "Epoch 83/500\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 99.5158 - mse: 99.5158\n",
      "Epoch 84/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 105.5566 - mse: 105.5566\n",
      "Epoch 85/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 95.8061 - mse: 95.8061\n",
      "Epoch 86/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 86.8340 - mse: 86.8340\n",
      "Epoch 87/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 88.3496 - mse: 88.3496\n",
      "Epoch 88/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 91.6235 - mse: 91.6235\n",
      "Epoch 89/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 80.6887 - mse: 80.6887\n",
      "Epoch 90/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 80.2014 - mse: 80.2014\n",
      "Epoch 91/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 81.8743 - mse: 81.8743\n",
      "Epoch 92/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 78.8428 - mse: 78.8428\n",
      "Epoch 93/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 90.6073 - mse: 90.6073\n",
      "Epoch 94/500\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 80.2314 - mse: 80.2314\n",
      "Epoch 95/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 92.7675 - mse: 92.7674\n",
      "Epoch 96/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 81.5743 - mse: 81.5743\n",
      "Epoch 97/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 99.1716 - mse: 99.1716\n",
      "Epoch 98/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 79.1423 - mse: 79.1423\n",
      "Epoch 99/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 73.5833 - mse: 73.5833\n",
      "Epoch 100/500\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 75.0616 - mse: 75.0616\n",
      "Epoch 101/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 85.4601 - mse: 85.4601\n",
      "Epoch 102/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 76.4611 - mse: 76.4611\n",
      "Epoch 103/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 70.6520 - mse: 70.6520\n",
      "Epoch 104/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 75.5061 - mse: 75.5061\n",
      "Epoch 105/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 96.4162 - mse: 96.4162\n",
      "Epoch 106/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 72.4231 - mse: 72.4231\n",
      "Epoch 107/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 68.1459 - mse: 68.1459\n",
      "Epoch 108/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 67.5928 - mse: 67.5928\n",
      "Epoch 109/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 80.9621 - mse: 80.9621\n",
      "Epoch 110/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 79.7268 - mse: 79.7268\n",
      "Epoch 111/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 66.7639 - mse: 66.7639\n",
      "Epoch 112/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 64.8694 - mse: 64.8694\n",
      "Epoch 113/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 64.9798 - mse: 64.9798\n",
      "Epoch 114/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 63.4777 - mse: 63.4777\n",
      "Epoch 115/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 65.4607 - mse: 65.4607\n",
      "Epoch 116/500\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 63.2395 - mse: 63.2395\n",
      "Epoch 117/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 64.3721 - mse: 64.3721\n",
      "Epoch 118/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 76.1155 - mse: 76.1155\n",
      "Epoch 119/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 61.4281 - mse: 61.4281\n",
      "Epoch 120/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 75.1603 - mse: 75.1603\n",
      "Epoch 121/500\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 62.9924 - mse: 62.9924\n",
      "Epoch 122/500\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 60.3415 - mse: 60.3415\n",
      "Epoch 123/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 61.9459 - mse: 61.9459\n",
      "Epoch 124/500\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 58.3590 - mse: 58.3590\n",
      "Epoch 125/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 73.0217 - mse: 73.0217\n",
      "Epoch 126/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 55.8604 - mse: 55.8604\n",
      "Epoch 127/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 64.9815 - mse: 64.9815\n",
      "Epoch 128/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 69.3076 - mse: 69.3076\n",
      "Epoch 129/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 59.9083 - mse: 59.9083\n",
      "Epoch 130/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 61.6197 - mse: 61.6197\n",
      "Epoch 131/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 64.7088 - mse: 64.7088\n",
      "Epoch 132/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 53.1999 - mse: 53.1999\n",
      "Epoch 133/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 54.7005 - mse: 54.7005\n",
      "Epoch 134/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 51.6212 - mse: 51.6212\n",
      "Epoch 135/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 47.2969 - mse: 47.296 - 0s 5ms/step - loss: 51.5355 - mse: 51.5355\n",
      "Epoch 136/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 50.7238 - mse: 50.7238\n",
      "Epoch 137/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 50.0761 - mse: 50.0761\n",
      "Epoch 138/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 50.0245 - mse: 50.0245\n",
      "Epoch 139/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 51.5025 - mse: 51.5025\n",
      "Epoch 140/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 51.1405 - mse: 51.1405\n",
      "Epoch 141/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 48.5828 - mse: 48.5828\n",
      "Epoch 142/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 45.5170 - mse: 45.5170\n",
      "Epoch 143/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 62.3927 - mse: 62.3927\n",
      "Epoch 144/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 49.9048 - mse: 49.9048\n",
      "Epoch 145/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 45.8281 - mse: 45.8281\n",
      "Epoch 146/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 47.5827 - mse: 47.5827\n",
      "Epoch 147/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 57.1562 - mse: 57.1562\n",
      "Epoch 148/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 54.4048 - mse: 54.4048\n",
      "Epoch 149/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 48.0234 - mse: 48.0234\n",
      "Epoch 150/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 42.9512 - mse: 42.9512\n",
      "Epoch 151/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 43.0221 - mse: 43.0221\n",
      "Epoch 152/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 47.6381 - mse: 47.6381\n",
      "Epoch 153/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 40.7911 - mse: 40.7911\n",
      "Epoch 154/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 40.6002 - mse: 40.6002\n",
      "Epoch 155/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 40.7914 - mse: 40.7914\n",
      "Epoch 156/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 64.6022 - mse: 64.6022\n",
      "Epoch 157/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 45.1082 - mse: 45.1082\n",
      "Epoch 158/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 41.4325 - mse: 41.4325\n",
      "Epoch 159/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 39.5041 - mse: 39.5041\n",
      "Epoch 160/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 38.0993 - mse: 38.0993\n",
      "Epoch 161/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 44.4027 - mse: 44.4027\n",
      "Epoch 162/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 40.9475 - mse: 40.9475\n",
      "Epoch 163/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 49.0840 - mse: 49.0840\n",
      "Epoch 164/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 44.2113 - mse: 44.2113\n",
      "Epoch 165/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 42.8000 - mse: 42.8000\n",
      "Epoch 166/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 41.1019 - mse: 41.1019\n",
      "Epoch 167/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 36.1821 - mse: 36.1821\n",
      "Epoch 168/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 47.3724 - mse: 47.3724\n",
      "Epoch 169/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 39.0974 - mse: 39.0974\n",
      "Epoch 170/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 35.1363 - mse: 35.1363\n",
      "Epoch 171/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 33.6953 - mse: 33.6953\n",
      "Epoch 172/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 33.7195 - mse: 33.7195\n",
      "Epoch 173/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 39.0532 - mse: 39.0532\n",
      "Epoch 174/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 34.9305 - mse: 34.9305\n",
      "Epoch 175/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 36.2192 - mse: 36.2192\n",
      "Epoch 176/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 42.2423 - mse: 42.2423\n",
      "Epoch 177/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 40.1838 - mse: 40.1838\n",
      "Epoch 178/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 31.2064 - mse: 31.2064\n",
      "Epoch 179/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 31.8024 - mse: 31.8024\n",
      "Epoch 180/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 34.9772 - mse: 34.9772\n",
      "Epoch 181/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 29.9829 - mse: 29.9829\n",
      "Epoch 182/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 29.2556 - mse: 29.2556\n",
      "Epoch 183/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 42.1018 - mse: 42.1018\n",
      "Epoch 184/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 43.3998 - mse: 43.3998\n",
      "Epoch 185/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 33.6008 - mse: 33.6008\n",
      "Epoch 186/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 29.6345 - mse: 29.6345\n",
      "Epoch 187/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 28.8529 - mse: 28.8529\n",
      "Epoch 188/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 28.2745 - mse: 28.2745\n",
      "Epoch 189/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 37.1115 - mse: 37.1115\n",
      "Epoch 190/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 29.1784 - mse: 29.1784\n",
      "Epoch 191/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 36.9155 - mse: 36.9155\n",
      "Epoch 192/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 34.1595 - mse: 34.1595\n",
      "Epoch 193/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 27.3231 - mse: 27.3231\n",
      "Epoch 194/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 32.1877 - mse: 32.1877\n",
      "Epoch 195/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 31.0582 - mse: 31.0582\n",
      "Epoch 196/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 26.3272 - mse: 26.3272\n",
      "Epoch 197/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 29.3943 - mse: 29.3943\n",
      "Epoch 198/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 32.2171 - mse: 32.2171\n",
      "Epoch 199/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 30.5824 - mse: 30.5824\n",
      "Epoch 200/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 31.5797 - mse: 31.5797\n",
      "Epoch 201/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 24.7643 - mse: 24.7643\n",
      "Epoch 202/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 27.6477 - mse: 27.6477\n",
      "Epoch 203/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 25.0288 - mse: 25.0288\n",
      "Epoch 204/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 27.5406 - mse: 27.5406\n",
      "Epoch 205/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 27.7920 - mse: 27.7920\n",
      "Epoch 206/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 22.6237 - mse: 22.6237\n",
      "Epoch 207/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 29.3139 - mse: 29.3139\n",
      "Epoch 208/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 24.0718 - mse: 24.0718\n",
      "Epoch 209/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 22.7825 - mse: 22.7825\n",
      "Epoch 210/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 38.9865 - mse: 38.9865\n",
      "Epoch 211/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 43.3167 - mse: 43.3167\n",
      "Epoch 212/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 22.9281 - mse: 22.9281\n",
      "Epoch 213/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 21.3449 - mse: 21.3449\n",
      "Epoch 214/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 20.9846 - mse: 20.9846\n",
      "Epoch 215/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 21.5814 - mse: 21.5814\n",
      "Epoch 216/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 22.6592 - mse: 22.6592\n",
      "Epoch 217/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 32.6817 - mse: 32.6817\n",
      "Epoch 218/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 30.8360 - mse: 30.8360\n",
      "Epoch 219/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 28.3820 - mse: 28.3820\n",
      "Epoch 220/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 19.9480 - mse: 19.9480\n",
      "Epoch 221/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 19.6278 - mse: 19.6278\n",
      "Epoch 222/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 21.5666 - mse: 21.5666\n",
      "Epoch 223/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 19.4795 - mse: 19.4795\n",
      "Epoch 224/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 18.6537 - mse: 18.6537\n",
      "Epoch 225/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 18.4924 - mse: 18.4924\n",
      "Epoch 226/500\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 19.9852 - mse: 19.9852\n",
      "Epoch 227/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 36.5012 - mse: 36.5012\n",
      "Epoch 228/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 20.1561 - mse: 20.1561\n",
      "Epoch 229/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 21.7316 - mse: 21.7316\n",
      "Epoch 230/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 24.6511 - mse: 24.6511\n",
      "Epoch 231/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 18.4910 - mse: 18.4910\n",
      "Epoch 232/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 16.9310 - mse: 16.9310\n",
      "Epoch 233/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 27.7902 - mse: 27.7902\n",
      "Epoch 234/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 32.4035 - mse: 32.4035\n",
      "Epoch 235/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 20.6740 - mse: 20.6740\n",
      "Epoch 236/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 19.1505 - mse: 19.1505\n",
      "Epoch 237/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 19.7585 - mse: 19.7585\n",
      "Epoch 238/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 18.4162 - mse: 18.4162\n",
      "Epoch 239/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 15.9277 - mse: 15.9277\n",
      "Epoch 240/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 17.0003 - mse: 17.0003\n",
      "Epoch 241/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 15.2701 - mse: 15.2701\n",
      "Epoch 242/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 15.7994 - mse: 15.7994\n",
      "Epoch 243/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 33.9935 - mse: 33.9935\n",
      "Epoch 244/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 25.0653 - mse: 25.0653\n",
      "Epoch 245/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 16.7436 - mse: 16.7436\n",
      "Epoch 246/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 17.3942 - mse: 17.3942\n",
      "Epoch 247/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 14.3503 - mse: 14.3503\n",
      "Epoch 248/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 17.3690 - mse: 17.3690\n",
      "Epoch 249/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 14.7362 - mse: 14.7362\n",
      "Epoch 250/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 13.9434 - mse: 13.9434\n",
      "Epoch 251/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 13.7904 - mse: 13.7904\n",
      "Epoch 252/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 20.4149 - mse: 20.4149\n",
      "Epoch 253/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 16.4295 - mse: 16.4295\n",
      "Epoch 254/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 13.1502 - mse: 13.1502\n",
      "Epoch 255/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 14.0416 - mse: 14.0416\n",
      "Epoch 256/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 22.7404 - mse: 22.7404\n",
      "Epoch 257/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 23.1744 - mse: 23.1744\n",
      "Epoch 258/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 20.6919 - mse: 20.6919\n",
      "Epoch 259/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 13.4355 - mse: 13.4355\n",
      "Epoch 260/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 12.2723 - mse: 12.2723\n",
      "Epoch 261/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 14.7272 - mse: 14.7272\n",
      "Epoch 262/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 18.2027 - mse: 18.2027\n",
      "Epoch 263/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 20.8534 - mse: 20.8534\n",
      "Epoch 264/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 20.1688 - mse: 20.1688\n",
      "Epoch 265/500\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 12.9693 - mse: 12.9693\n",
      "Epoch 266/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 14.8936 - mse: 14.8936\n",
      "Epoch 267/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 11.6660 - mse: 11.6660\n",
      "Epoch 268/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 16.3502 - mse: 16.3502\n",
      "Epoch 269/500\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 20.7991 - mse: 20.7991\n",
      "Epoch 270/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 12.2863 - mse: 12.2863\n",
      "Epoch 271/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 16.1347 - mse: 16.1347\n",
      "Epoch 272/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 18.8836 - mse: 18.8836\n",
      "Epoch 273/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 10.7537 - mse: 10.7537\n",
      "Epoch 274/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 14.6879 - mse: 14.6879\n",
      "Epoch 275/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 11.7459 - mse: 11.7459\n",
      "Epoch 276/500\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 13.1117 - mse: 13.1117\n",
      "Epoch 277/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 11.1177 - mse: 11.1177\n",
      "Epoch 278/500\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 10.9733 - mse: 10.9733\n",
      "Epoch 279/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 10.3378 - mse: 10.3378\n",
      "Epoch 280/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 9.7299 - mse: 9.7299\n",
      "Epoch 281/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 9.7390 - mse: 9.7390\n",
      "Epoch 282/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 11.0916 - mse: 11.0916\n",
      "Epoch 283/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 22.9887 - mse: 22.9887\n",
      "Epoch 284/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 24.3792 - mse: 24.3792\n",
      "Epoch 285/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 16.4101 - mse: 16.4101\n",
      "Epoch 286/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 14.3481 - mse: 14.3481\n",
      "Epoch 287/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 9.7118 - mse: 9.7118\n",
      "Epoch 288/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 8.8976 - mse: 8.8976\n",
      "Epoch 289/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 12.2403 - mse: 12.2403\n",
      "Epoch 290/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 11.4705 - mse: 11.4705\n",
      "Epoch 291/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 12.6391 - mse: 12.6391\n",
      "Epoch 292/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 12.3997 - mse: 12.3997\n",
      "Epoch 293/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 13.5533 - mse: 13.5533\n",
      "Epoch 294/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 11.5234 - mse: 11.5234\n",
      "Epoch 295/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 8.8210 - mse: 8.8210\n",
      "Epoch 296/500\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 10.6604 - mse: 10.6604\n",
      "Epoch 297/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 10.3755 - mse: 10.3754\n",
      "Epoch 298/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 10.2772 - mse: 10.2772\n",
      "Epoch 299/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 12.2714 - mse: 12.2714\n",
      "Epoch 300/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 9.7237 - mse: 9.7237\n",
      "Epoch 301/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 7.4028 - mse: 7.4028\n",
      "Epoch 302/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 11.7643 - mse: 11.7643\n",
      "Epoch 303/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 7.3725 - mse: 7.3725\n",
      "Epoch 304/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 15.0763 - mse: 15.0763\n",
      "Epoch 305/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 26.2493 - mse: 26.2493\n",
      "Epoch 306/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 10.0251 - mse: 10.0251\n",
      "Epoch 307/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 8.6553 - mse: 8.6553\n",
      "Epoch 308/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 7.0249 - mse: 7.0249\n",
      "Epoch 309/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 7.0183 - mse: 7.0183\n",
      "Epoch 310/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 7.1014 - mse: 7.1014\n",
      "Epoch 311/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 7.6440 - mse: 7.6440\n",
      "Epoch 312/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 7.3099 - mse: 7.3099\n",
      "Epoch 313/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 12.0850 - mse: 12.0850\n",
      "Epoch 314/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 18.9520 - mse: 18.9520\n",
      "Epoch 315/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 10.7347 - mse: 10.7347\n",
      "Epoch 316/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 8.3377 - mse: 8.3377\n",
      "Epoch 317/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 6.3338 - mse: 6.3338\n",
      "Epoch 318/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 8.3986 - mse: 8.3986\n",
      "Epoch 319/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 18.9933 - mse: 18.9933\n",
      "Epoch 320/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 9.3866 - mse: 9.3866\n",
      "Epoch 321/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 6.1503 - mse: 6.1503\n",
      "Epoch 322/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 6.2201 - mse: 6.2201\n",
      "Epoch 323/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 5.9212 - mse: 5.9212\n",
      "Epoch 324/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 5.8779 - mse: 5.8779\n",
      "Epoch 325/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 5.9011 - mse: 5.9011\n",
      "Epoch 326/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 16.2404 - mse: 16.2404\n",
      "Epoch 327/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 11.7753 - mse: 11.7753\n",
      "Epoch 328/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 5.5524 - mse: 5.5524\n",
      "Epoch 329/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 7.7181 - mse: 7.7181\n",
      "Epoch 330/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 13.3717 - mse: 13.3717\n",
      "Epoch 331/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 14.1915 - mse: 14.1915\n",
      "Epoch 332/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 6.7057 - mse: 6.7057\n",
      "Epoch 333/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 5.7416 - mse: 5.7416\n",
      "Epoch 334/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 5.5494 - mse: 5.5494\n",
      "Epoch 335/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 5.6689 - mse: 5.6689\n",
      "Epoch 336/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 6.1625 - mse: 6.1625\n",
      "Epoch 337/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 7.4809 - mse: 7.4809\n",
      "Epoch 338/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 7.7677 - mse: 7.7677\n",
      "Epoch 339/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 13.7000 - mse: 13.7000\n",
      "Epoch 340/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 11.6871 - mse: 11.6871\n",
      "Epoch 341/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 5.5732 - mse: 5.5732\n",
      "Epoch 342/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 5.4985 - mse: 5.4985\n",
      "Epoch 343/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 6.7756 - mse: 6.7756\n",
      "Epoch 344/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 9.0023 - mse: 9.0023\n",
      "Epoch 345/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 9.8186 - mse: 9.8186\n",
      "Epoch 346/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 11.0597 - mse: 11.0597\n",
      "Epoch 347/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 4.7013 - mse: 4.7013\n",
      "Epoch 348/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 4.6650 - mse: 4.6650\n",
      "Epoch 349/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 4.5221 - mse: 4.5221\n",
      "Epoch 350/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 4.9504 - mse: 4.9504\n",
      "Epoch 351/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 10.1447 - mse: 10.1447\n",
      "Epoch 352/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 24.6910 - mse: 24.6910\n",
      "Epoch 353/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 13.8075 - mse: 13.8075\n",
      "Epoch 354/500\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 4.6896 - mse: 4.6896\n",
      "Epoch 355/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 4.2918 - mse: 4.2918\n",
      "Epoch 356/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 4.3229 - mse: 4.3229\n",
      "Epoch 357/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 4.3027 - mse: 4.3027\n",
      "Epoch 358/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 4.1186 - mse: 4.1186\n",
      "Epoch 359/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 4.6990 - mse: 4.6990\n",
      "Epoch 360/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 6.3824 - mse: 6.3824\n",
      "Epoch 361/500\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 14.7222 - mse: 14.7222\n",
      "Epoch 362/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 15.6879 - mse: 15.6879\n",
      "Epoch 363/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 6.1271 - mse: 6.1271\n",
      "Epoch 364/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 3.9607 - mse: 3.9607\n",
      "Epoch 365/500\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 5.4756 - mse: 5.4756\n",
      "Epoch 366/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 14.0790 - mse: 14.0790\n",
      "Epoch 367/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 9.3254 - mse: 9.3254\n",
      "Epoch 368/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 4.3428 - mse: 4.3428\n",
      "Epoch 369/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 3.8124 - mse: 3.8124\n",
      "Epoch 370/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 3.7936 - mse: 3.7936\n",
      "Epoch 371/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 3.8829 - mse: 3.8829\n",
      "Epoch 372/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 3.7105 - mse: 3.7105\n",
      "Epoch 373/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 3.4742 - mse: 3.4742\n",
      "Epoch 374/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 3.8396 - mse: 3.8396\n",
      "Epoch 375/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 7.0486 - mse: 7.0486\n",
      "Epoch 376/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 28.3864 - mse: 28.3864\n",
      "Epoch 377/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 10.8652 - mse: 10.8652\n",
      "Epoch 378/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 4.0884 - mse: 4.0884\n",
      "Epoch 379/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 4.2004 - mse: 4.2004\n",
      "Epoch 380/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 3.7426 - mse: 3.7426\n",
      "Epoch 381/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 3.3339 - mse: 3.3339\n",
      "Epoch 382/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 3.3748 - mse: 3.3748\n",
      "Epoch 383/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 5.4038 - mse: 5.4038\n",
      "Epoch 384/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 16.9472 - mse: 16.9472\n",
      "Epoch 385/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 13.0158 - mse: 13.0158\n",
      "Epoch 386/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 6.3672 - mse: 6.3672\n",
      "Epoch 387/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 3.3553 - mse: 3.3553\n",
      "Epoch 388/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 3.5812 - mse: 3.5812\n",
      "Epoch 389/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 3.2730 - mse: 3.2730\n",
      "Epoch 390/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 3.1549 - mse: 3.1549\n",
      "Epoch 391/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 4.4418 - mse: 4.4418\n",
      "Epoch 392/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 8.6195 - mse: 8.6195\n",
      "Epoch 393/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 19.9710 - mse: 19.9710\n",
      "Epoch 394/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 11.5011 - mse: 11.5011\n",
      "Epoch 395/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 4.4012 - mse: 4.4012\n",
      "Epoch 396/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 4.0278 - mse: 4.0278\n",
      "Epoch 397/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 2.9561 - mse: 2.9561\n",
      "Epoch 398/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 3.3032 - mse: 3.3032\n",
      "Epoch 399/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 4.2970 - mse: 4.2970\n",
      "Epoch 400/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 4.0335 - mse: 4.0335\n",
      "Epoch 401/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 6.0870 - mse: 6.0870\n",
      "Epoch 402/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 19.2470 - mse: 19.2470\n",
      "Epoch 403/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 8.5125 - mse: 8.5125\n",
      "Epoch 404/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 3.5437 - mse: 3.5437\n",
      "Epoch 405/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 3.2707 - mse: 3.2707\n",
      "Epoch 406/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 3.2633 - mse: 3.2633\n",
      "Epoch 407/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 4.8109 - mse: 4.8109\n",
      "Epoch 408/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 10.3268 - mse: 10.3268\n",
      "Epoch 409/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 11.3294 - mse: 11.3294\n",
      "Epoch 410/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 5.1040 - mse: 5.1040\n",
      "Epoch 411/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 3.1329 - mse: 3.1329\n",
      "Epoch 412/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 2.6222 - mse: 2.6222\n",
      "Epoch 413/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 4.0750 - mse: 4.0750\n",
      "Epoch 414/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 9.1586 - mse: 9.1586\n",
      "Epoch 415/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 15.9061 - mse: 15.9061\n",
      "Epoch 416/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 11.9764 - mse: 11.9764\n",
      "Epoch 417/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 4.2816 - mse: 4.2816\n",
      "Epoch 418/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 2.5023 - mse: 2.5023\n",
      "Epoch 419/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 3.2940 - mse: 3.2940\n",
      "Epoch 420/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 4.1467 - mse: 4.1467\n",
      "Epoch 421/500\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 4.3676 - mse: 4.3676\n",
      "Epoch 422/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 5.4168 - mse: 5.4168\n",
      "Epoch 423/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 8.2831 - mse: 8.2831\n",
      "Epoch 424/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 14.3122 - mse: 14.3122\n",
      "Epoch 425/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 3.8647 - mse: 3.8647\n",
      "Epoch 426/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 3.7705 - mse: 3.7705\n",
      "Epoch 427/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 4.5945 - mse: 4.5945\n",
      "Epoch 428/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 3.2163 - mse: 3.2163\n",
      "Epoch 429/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 4.9291 - mse: 4.9291\n",
      "Epoch 430/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 6.3073 - mse: 6.3073\n",
      "Epoch 431/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 6.2577 - mse: 6.2577\n",
      "Epoch 432/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 7.8310 - mse: 7.8310\n",
      "Epoch 433/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 6.4283 - mse: 6.4283\n",
      "Epoch 434/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 5.1847 - mse: 5.1847\n",
      "Epoch 435/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 5.6462 - mse: 5.6462\n",
      "Epoch 436/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 6.6671 - mse: 6.6671\n",
      "Epoch 437/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 8.5989 - mse: 8.5989\n",
      "Epoch 438/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 6.7451 - mse: 6.7451\n",
      "Epoch 439/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 3.2046 - mse: 3.2046\n",
      "Epoch 440/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 4.0013 - mse: 4.0013\n",
      "Epoch 441/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 3.8058 - mse: 3.8058\n",
      "Epoch 442/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 5.2646 - mse: 5.2646\n",
      "Epoch 443/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 9.6221 - mse: 9.6221\n",
      "Epoch 444/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 10.9125 - mse: 10.9125\n",
      "Epoch 445/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 3.6480 - mse: 3.6480\n",
      "Epoch 446/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 5.4105 - mse: 5.4105\n",
      "Epoch 447/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 7.4781 - mse: 7.4781\n",
      "Epoch 448/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 7.2521 - mse: 7.2521\n",
      "Epoch 449/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 3.3012 - mse: 3.3012\n",
      "Epoch 450/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 2.0822 - mse: 2.0822\n",
      "Epoch 451/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 3.8233 - mse: 3.8233\n",
      "Epoch 452/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 10.9744 - mse: 10.9744\n",
      "Epoch 453/500\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 8.4208 - mse: 8.4208\n",
      "Epoch 454/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 4.0289 - mse: 4.0289\n",
      "Epoch 455/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 3.8283 - mse: 3.8283\n",
      "Epoch 456/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 4.5513 - mse: 4.5513\n",
      "Epoch 457/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 4.2046 - mse: 4.2046\n",
      "Epoch 458/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 4.4191 - mse: 4.4191\n",
      "Epoch 459/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 7.4552 - mse: 7.4552\n",
      "Epoch 460/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 8.0378 - mse: 8.0378\n",
      "Epoch 461/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 3.2320 - mse: 3.2320\n",
      "Epoch 462/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 3.5582 - mse: 3.5582\n",
      "Epoch 463/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 4.8120 - mse: 4.8120\n",
      "Epoch 464/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 5.0133 - mse: 5.0133\n",
      "Epoch 465/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 9.5644 - mse: 9.5644\n",
      "Epoch 466/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 12.6505 - mse: 12.6505\n",
      "Epoch 467/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 10.2223 - mse: 10.2223\n",
      "Epoch 468/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 5.5118 - mse: 5.5118\n",
      "Epoch 469/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 4.2395 - mse: 4.2395\n",
      "Epoch 470/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 2.5296 - mse: 2.5296\n",
      "Epoch 471/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 2.4247 - mse: 2.4247\n",
      "Epoch 472/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 2.0685 - mse: 2.0685\n",
      "Epoch 473/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 1.8666 - mse: 1.8666\n",
      "Epoch 474/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 2.1082 - mse: 2.1082\n",
      "Epoch 475/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 4.4154 - mse: 4.4154\n",
      "Epoch 476/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 13.0321 - mse: 13.0321\n",
      "Epoch 477/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 11.2534 - mse: 11.2534\n",
      "Epoch 478/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 8.2802 - mse: 8.2802\n",
      "Epoch 479/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 3.2470 - mse: 3.2470\n",
      "Epoch 480/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 2.1695 - mse: 2.1695\n",
      "Epoch 481/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 1.8447 - mse: 1.8447\n",
      "Epoch 482/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 4.8592 - mse: 4.8592\n",
      "Epoch 483/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 11.5958 - mse: 11.5958\n",
      "Epoch 484/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 11.7658 - mse: 11.7658\n",
      "Epoch 485/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 5.7210 - mse: 5.7210\n",
      "Epoch 486/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 3.1086 - mse: 3.1086\n",
      "Epoch 487/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 4.3529 - mse: 4.3529\n",
      "Epoch 488/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 2.7530 - mse: 2.7530\n",
      "Epoch 489/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 2.0460 - mse: 2.0460\n",
      "Epoch 490/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 2.4765 - mse: 2.4765\n",
      "Epoch 491/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 6.1326 - mse: 6.1326\n",
      "Epoch 492/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 8.2353 - mse: 8.2353\n",
      "Epoch 493/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 8.6749 - mse: 8.6749\n",
      "Epoch 494/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 8.2986 - mse: 8.2986\n",
      "Epoch 495/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 4.0099 - mse: 4.0099\n",
      "Epoch 496/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 4.2705 - mse: 4.2705\n",
      "Epoch 497/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 2.7529 - mse: 2.7529\n",
      "Epoch 498/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 1.9523 - mse: 1.9523\n",
      "Epoch 499/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 1.7421 - mse: 1.7421\n",
      "Epoch 500/500\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 1.6311 - mse: 1.6311\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x295b204c0>"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X, y, epochs=500, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Observe the numbers for the `weights`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[ 0.22794743,  0.18892403,  0.03855004],\n",
       "        [ 0.18504235, -0.6992631 , -0.17051505],\n",
       "        [ 0.23789686, -0.6650393 , -0.6229587 ],\n",
       "        [ 0.07445751, -0.46606123,  0.60378456],\n",
       "        [ 0.06755638, -0.6152374 , -0.44378594],\n",
       "        [-0.17589214, -0.56580186, -0.2693516 ]], dtype=float32),\n",
       " array([ 0.09451145, -0.09535529,  0.09451766], dtype=float32),\n",
       " array([[ 0.7423031],\n",
       "        [-0.7008662],\n",
       "        [ 1.0805109]], dtype=float32),\n",
       " array([0.09480259], dtype=float32)]"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.get_weights()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### Predictions vs Reality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 1. Calculate the Predicted Accidents and\n",
    "> 2. Compare it with the Real Total Accidents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-02 11:29:08.192308: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>total</th>\n",
       "      <th>pred_init_1</th>\n",
       "      <th>pred_1_after_fit</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>abbrev</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>AL</th>\n",
       "      <td>18.8</td>\n",
       "      <td>-257.937042</td>\n",
       "      <td>16.217836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AK</th>\n",
       "      <td>18.1</td>\n",
       "      <td>-307.893494</td>\n",
       "      <td>18.597555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AZ</th>\n",
       "      <td>18.6</td>\n",
       "      <td>-255.119736</td>\n",
       "      <td>20.028687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AR</th>\n",
       "      <td>22.4</td>\n",
       "      <td>-259.586212</td>\n",
       "      <td>22.548859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CA</th>\n",
       "      <td>12.0</td>\n",
       "      <td>-301.841125</td>\n",
       "      <td>10.325897</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        total  pred_init_1  pred_1_after_fit\n",
       "abbrev                                      \n",
       "AL       18.8  -257.937042         16.217836\n",
       "AK       18.1  -307.893494         18.597555\n",
       "AZ       18.6  -255.119736         20.028687\n",
       "AR       22.4  -259.586212         22.548859\n",
       "CA       12.0  -301.841125         10.325897"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfsel['pred_1_after_fit'] = y_pred\n",
    "dfsel.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.9552221989548562"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mse = ((dfsel.total - dfsel.pred_1_after_fit)**2).mean()\n",
    "mse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Play with the Activation Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - https://keras.io/api/layers/activations/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/IHZwWFHWa-w?start=558\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%HTML\n",
    "<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/IHZwWFHWa-w?start=558\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use `sigmoid` activation in last layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(layer=Input(shape=(6,)))\n",
    "model.add(layer=Dense(units=3, kernel_initializer='glorot_uniform'))\n",
    "model.add(layer=Dense(units=1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='mse', metrics=['mse'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `fit()` the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-02 11:29:12.394531: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2a43c6ca0>"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X, y, epochs=500, verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predictions vs Reality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 1. Calculate the Predicted Accidents and\n",
    "> 2. Compare it with the Real Total Accidents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-02 11:29:16.576044: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>total</th>\n",
       "      <th>pred_init_1</th>\n",
       "      <th>pred_1_after_fit</th>\n",
       "      <th>pred_sigmoid</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>abbrev</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>AL</th>\n",
       "      <td>18.8</td>\n",
       "      <td>-257.937042</td>\n",
       "      <td>16.217836</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AK</th>\n",
       "      <td>18.1</td>\n",
       "      <td>-307.893494</td>\n",
       "      <td>18.597555</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AZ</th>\n",
       "      <td>18.6</td>\n",
       "      <td>-255.119736</td>\n",
       "      <td>20.028687</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AR</th>\n",
       "      <td>22.4</td>\n",
       "      <td>-259.586212</td>\n",
       "      <td>22.548859</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CA</th>\n",
       "      <td>12.0</td>\n",
       "      <td>-301.841125</td>\n",
       "      <td>10.325897</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        total  pred_init_1  pred_1_after_fit  pred_sigmoid\n",
       "abbrev                                                    \n",
       "AL       18.8  -257.937042         16.217836           1.0\n",
       "AK       18.1  -307.893494         18.597555           1.0\n",
       "AZ       18.6  -255.119736         20.028687           1.0\n",
       "AR       22.4  -259.586212         22.548859           1.0\n",
       "CA       12.0  -301.841125         10.325897           1.0"
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfsel['pred_sigmoid'] = y_pred\n",
    "dfsel.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "235.4076470588235"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mse = ((dfsel.total - dfsel.pred_sigmoid)**2).mean()\n",
    "mse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observe the numbers for the `weights`\n",
    "\n",
    "> - Have they changed?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[-0.05230647, -0.28056693, -0.79694104],\n",
       "        [-0.5071227 ,  0.5546968 ,  0.22677517],\n",
       "        [-0.11774123,  0.18490088, -0.6657125 ],\n",
       "        [ 0.2158922 , -0.70551896, -0.39937165],\n",
       "        [ 0.08159274, -0.48229125, -0.5145814 ],\n",
       "        [-0.8022857 , -0.02043778, -0.52640724]], dtype=float32),\n",
       " array([0., 0., 0.], dtype=float32),\n",
       " array([[-0.32881755],\n",
       "        [ 0.10825825],\n",
       "        [-0.80588156]], dtype=float32),\n",
       " array([0.], dtype=float32)]"
      ]
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.get_weights()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use `linear` activation in last layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(layer=Input(shape=(6,)))\n",
    "model.add(layer=Dense(units=3, kernel_initializer='glorot_uniform'))\n",
    "model.add(layer=Dense(units=1, activation='linear'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='mse', metrics=['mse'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `fit()` the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-02 11:29:19.464303: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x295b8e2b0>"
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X, y, epochs=500, verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predictions vs Reality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 1. Calculate the Predicted Accidents and\n",
    "> 2. Compare it with the Real Total Accidents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-02 11:29:23.571089: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>total</th>\n",
       "      <th>pred_init_1</th>\n",
       "      <th>pred_1_after_fit</th>\n",
       "      <th>pred_sigmoid</th>\n",
       "      <th>pred_linear</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>abbrev</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>AL</th>\n",
       "      <td>18.8</td>\n",
       "      <td>-257.937042</td>\n",
       "      <td>16.217836</td>\n",
       "      <td>1.0</td>\n",
       "      <td>23.283497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AK</th>\n",
       "      <td>18.1</td>\n",
       "      <td>-307.893494</td>\n",
       "      <td>18.597555</td>\n",
       "      <td>1.0</td>\n",
       "      <td>12.349394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AZ</th>\n",
       "      <td>18.6</td>\n",
       "      <td>-255.119736</td>\n",
       "      <td>20.028687</td>\n",
       "      <td>1.0</td>\n",
       "      <td>8.547904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AR</th>\n",
       "      <td>22.4</td>\n",
       "      <td>-259.586212</td>\n",
       "      <td>22.548859</td>\n",
       "      <td>1.0</td>\n",
       "      <td>18.549000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CA</th>\n",
       "      <td>12.0</td>\n",
       "      <td>-301.841125</td>\n",
       "      <td>10.325897</td>\n",
       "      <td>1.0</td>\n",
       "      <td>27.273436</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        total  pred_init_1  pred_1_after_fit  pred_sigmoid  pred_linear\n",
       "abbrev                                                                 \n",
       "AL       18.8  -257.937042         16.217836           1.0    23.283497\n",
       "AK       18.1  -307.893494         18.597555           1.0    12.349394\n",
       "AZ       18.6  -255.119736         20.028687           1.0     8.547904\n",
       "AR       22.4  -259.586212         22.548859           1.0    18.549000\n",
       "CA       12.0  -301.841125         10.325897           1.0    27.273436"
      ]
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfsel['pred_linear'] = y_pred\n",
    "dfsel.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "235.4076470588235"
      ]
     },
     "execution_count": 233,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mse = ((dfsel.total - dfsel.pred_sigmoid)**2).mean()\n",
    "mse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observe the numbers for the `weights`\n",
    "\n",
    "> - Have they changed?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[-1.031418  ,  0.29482067, -0.10314092],\n",
       "        [ 0.34446746, -0.34492567, -0.41608915],\n",
       "        [ 0.4260584 , -0.05254743, -0.7435918 ],\n",
       "        [ 0.17749521, -0.11258825,  0.13359837],\n",
       "        [ 0.18976219,  0.09851412,  0.1697132 ],\n",
       "        [-0.55843914,  0.10069706, -0.7959105 ]], dtype=float32),\n",
       " array([-0.22634532,  0.22730479, -0.22720319], dtype=float32),\n",
       " array([[-0.3127318 ],\n",
       "        [ 0.513088  ],\n",
       "        [-0.11429369]], dtype=float32),\n",
       " array([0.22826387], dtype=float32)]"
      ]
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.get_weights()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use `tanh` activation in last layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(layer=Input(shape=(6,)))\n",
    "model.add(layer=Dense(units=3, kernel_initializer='glorot_uniform'))\n",
    "model.add(layer=Dense(units=1, activation='tanh'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='mse', metrics=['mse'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `fit()` the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-02 11:29:53.044504: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2a8825370>"
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X, y, epochs=500, verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predictions vs Reality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 1. Calculate the Predicted Accidents and\n",
    "> 2. Compare it with the Real Total Accidents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-02 11:29:57.176531: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>total</th>\n",
       "      <th>pred_init_1</th>\n",
       "      <th>pred_1_after_fit</th>\n",
       "      <th>pred_sigmoid</th>\n",
       "      <th>pred_linear</th>\n",
       "      <th>pred_tanh</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>abbrev</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>AL</th>\n",
       "      <td>18.8</td>\n",
       "      <td>-257.937042</td>\n",
       "      <td>16.217836</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AK</th>\n",
       "      <td>18.1</td>\n",
       "      <td>-307.893494</td>\n",
       "      <td>18.597555</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AZ</th>\n",
       "      <td>18.6</td>\n",
       "      <td>-255.119736</td>\n",
       "      <td>20.028687</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AR</th>\n",
       "      <td>22.4</td>\n",
       "      <td>-259.586212</td>\n",
       "      <td>22.548859</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CA</th>\n",
       "      <td>12.0</td>\n",
       "      <td>-301.841125</td>\n",
       "      <td>10.325897</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        total  pred_init_1  pred_1_after_fit  pred_sigmoid  pred_linear  \\\n",
       "abbrev                                                                    \n",
       "AL       18.8  -257.937042         16.217836           1.0          1.0   \n",
       "AK       18.1  -307.893494         18.597555           1.0          1.0   \n",
       "AZ       18.6  -255.119736         20.028687           1.0          1.0   \n",
       "AR       22.4  -259.586212         22.548859           1.0          1.0   \n",
       "CA       12.0  -301.841125         10.325897           1.0          1.0   \n",
       "\n",
       "        pred_tanh  \n",
       "abbrev             \n",
       "AL            1.0  \n",
       "AK            1.0  \n",
       "AZ            1.0  \n",
       "AR            1.0  \n",
       "CA            1.0  "
      ]
     },
     "execution_count": 241,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfsel['pred_tanh'] = y_pred\n",
    "dfsel.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "235.4076470588235"
      ]
     },
     "execution_count": 242,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mse = ((dfsel.total - dfsel.pred_sigmoid)**2).mean()\n",
    "mse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observe the numbers for the `weights`\n",
    "\n",
    "> - Have they changed?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[-1.031418  ,  0.29482067, -0.10314092],\n",
       "        [ 0.34446746, -0.34492567, -0.41608915],\n",
       "        [ 0.4260584 , -0.05254743, -0.7435918 ],\n",
       "        [ 0.17749521, -0.11258825,  0.13359837],\n",
       "        [ 0.18976219,  0.09851412,  0.1697132 ],\n",
       "        [-0.55843914,  0.10069706, -0.7959105 ]], dtype=float32),\n",
       " array([-0.22634532,  0.22730479, -0.22720319], dtype=float32),\n",
       " array([[-0.3127318 ],\n",
       "        [ 0.513088  ],\n",
       "        [-0.11429369]], dtype=float32),\n",
       " array([0.22826387], dtype=float32)]"
      ]
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use `relu` activation in last layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(layer=Input(shape=(6,)))\n",
    "model.add(layer=Dense(units=3, kernel_initializer='glorot_uniform'))\n",
    "model.add(layer=Dense(units=1, activation='relu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='mse', metrics=['mse'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `fit()` the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-02 11:32:16.748880: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x177f0adf0>"
      ]
     },
     "execution_count": 245,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X, y, epochs=500, verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predictions vs Reality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 1. Calculate the Predicted Accidents and\n",
    "> 2. Compare it with the Real Total Accidents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-02 11:32:22.374663: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>total</th>\n",
       "      <th>pred_init_1</th>\n",
       "      <th>pred_1_after_fit</th>\n",
       "      <th>pred_sigmoid</th>\n",
       "      <th>pred_linear</th>\n",
       "      <th>pred_tanh</th>\n",
       "      <th>pred_relu</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>abbrev</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>AL</th>\n",
       "      <td>18.8</td>\n",
       "      <td>-257.937042</td>\n",
       "      <td>16.217836</td>\n",
       "      <td>1.0</td>\n",
       "      <td>14.564026</td>\n",
       "      <td>1.0</td>\n",
       "      <td>14.564026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AK</th>\n",
       "      <td>18.1</td>\n",
       "      <td>-307.893494</td>\n",
       "      <td>18.597555</td>\n",
       "      <td>1.0</td>\n",
       "      <td>16.471922</td>\n",
       "      <td>1.0</td>\n",
       "      <td>16.471922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AZ</th>\n",
       "      <td>18.6</td>\n",
       "      <td>-255.119736</td>\n",
       "      <td>20.028687</td>\n",
       "      <td>1.0</td>\n",
       "      <td>17.129185</td>\n",
       "      <td>1.0</td>\n",
       "      <td>17.129185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AR</th>\n",
       "      <td>22.4</td>\n",
       "      <td>-259.586212</td>\n",
       "      <td>22.548859</td>\n",
       "      <td>1.0</td>\n",
       "      <td>19.351301</td>\n",
       "      <td>1.0</td>\n",
       "      <td>19.351301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CA</th>\n",
       "      <td>12.0</td>\n",
       "      <td>-301.841125</td>\n",
       "      <td>10.325897</td>\n",
       "      <td>1.0</td>\n",
       "      <td>9.687492</td>\n",
       "      <td>1.0</td>\n",
       "      <td>9.687492</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        total  pred_init_1  pred_1_after_fit  pred_sigmoid  pred_linear  \\\n",
       "abbrev                                                                    \n",
       "AL       18.8  -257.937042         16.217836           1.0    14.564026   \n",
       "AK       18.1  -307.893494         18.597555           1.0    16.471922   \n",
       "AZ       18.6  -255.119736         20.028687           1.0    17.129185   \n",
       "AR       22.4  -259.586212         22.548859           1.0    19.351301   \n",
       "CA       12.0  -301.841125         10.325897           1.0     9.687492   \n",
       "\n",
       "        pred_tanh  pred_relu  \n",
       "abbrev                        \n",
       "AL            1.0  14.564026  \n",
       "AK            1.0  16.471922  \n",
       "AZ            1.0  17.129185  \n",
       "AR            1.0  19.351301  \n",
       "CA            1.0   9.687492  "
      ]
     },
     "execution_count": 248,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfsel['pred_relu'] = y_pred\n",
    "dfsel.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "235.4076470588235"
      ]
     },
     "execution_count": 249,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mse = ((dfsel.total - dfsel.pred_sigmoid)**2).mean()\n",
    "mse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observe the numbers for the `weights`\n",
    "\n",
    "> - Have they changed?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[ 0.36294058, -0.38705733, -0.33608356],\n",
       "        [ 0.25670263, -1.0491729 ,  0.9450764 ],\n",
       "        [-0.13583986, -0.3517214 ,  0.03684956],\n",
       "        [-0.9108256 , -0.7394024 ,  0.98108435],\n",
       "        [-0.4182683 ,  0.46755537,  0.31544292],\n",
       "        [-0.5613835 ,  0.36370704, -0.20064451]], dtype=float32),\n",
       " array([-0.0409128 , -0.02671915,  0.03969008], dtype=float32),\n",
       " array([[-0.23136768],\n",
       "        [-0.3219077 ],\n",
       "        [ 0.18412863]], dtype=float32),\n",
       " array([0.03613757], dtype=float32)]"
      ]
     },
     "execution_count": 250,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.get_weights()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How are the predictions changing? Why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(layer=Input(shape=(6,)))\n",
    "model.add(layer=Dense(units=3, kernel_initializer='glorot_uniform', activation='relu'))\n",
    "model.add(layer=Dense(units=1, activation='linear'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='mse', metrics=['mse'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `fit()` the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-02 11:33:34.415602: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x175e64400>"
      ]
     },
     "execution_count": 255,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X, y, epochs=500, verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predictions vs Reality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 1. Calculate the Predicted Accidents and\n",
    "> 2. Compare it with the Real Total Accidents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-02 11:33:39.363718: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>total</th>\n",
       "      <th>pred_init_1</th>\n",
       "      <th>pred_1_after_fit</th>\n",
       "      <th>pred_sigmoid</th>\n",
       "      <th>pred_linear</th>\n",
       "      <th>pred_tanh</th>\n",
       "      <th>pred_relu</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>abbrev</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>AL</th>\n",
       "      <td>18.8</td>\n",
       "      <td>-257.937042</td>\n",
       "      <td>16.217836</td>\n",
       "      <td>1.0</td>\n",
       "      <td>18.319891</td>\n",
       "      <td>1.0</td>\n",
       "      <td>14.564026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AK</th>\n",
       "      <td>18.1</td>\n",
       "      <td>-307.893494</td>\n",
       "      <td>18.597555</td>\n",
       "      <td>1.0</td>\n",
       "      <td>16.408112</td>\n",
       "      <td>1.0</td>\n",
       "      <td>16.471922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AZ</th>\n",
       "      <td>18.6</td>\n",
       "      <td>-255.119736</td>\n",
       "      <td>20.028687</td>\n",
       "      <td>1.0</td>\n",
       "      <td>16.703562</td>\n",
       "      <td>1.0</td>\n",
       "      <td>17.129185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AR</th>\n",
       "      <td>22.4</td>\n",
       "      <td>-259.586212</td>\n",
       "      <td>22.548859</td>\n",
       "      <td>1.0</td>\n",
       "      <td>18.824598</td>\n",
       "      <td>1.0</td>\n",
       "      <td>19.351301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CA</th>\n",
       "      <td>12.0</td>\n",
       "      <td>-301.841125</td>\n",
       "      <td>10.325897</td>\n",
       "      <td>1.0</td>\n",
       "      <td>11.142485</td>\n",
       "      <td>1.0</td>\n",
       "      <td>9.687492</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        total  pred_init_1  pred_1_after_fit  pred_sigmoid  pred_linear  \\\n",
       "abbrev                                                                    \n",
       "AL       18.8  -257.937042         16.217836           1.0    18.319891   \n",
       "AK       18.1  -307.893494         18.597555           1.0    16.408112   \n",
       "AZ       18.6  -255.119736         20.028687           1.0    16.703562   \n",
       "AR       22.4  -259.586212         22.548859           1.0    18.824598   \n",
       "CA       12.0  -301.841125         10.325897           1.0    11.142485   \n",
       "\n",
       "        pred_tanh  pred_relu  \n",
       "abbrev                        \n",
       "AL            1.0  14.564026  \n",
       "AK            1.0  16.471922  \n",
       "AZ            1.0  17.129185  \n",
       "AR            1.0  19.351301  \n",
       "CA            1.0   9.687492  "
      ]
     },
     "execution_count": 257,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfsel['pred_linear'] = y_pred\n",
    "dfsel.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "235.4076470588235"
      ]
     },
     "execution_count": 258,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mse = ((dfsel.total - dfsel.pred_sigmoid)**2).mean()\n",
    "mse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observe the numbers for the `weights`\n",
    "\n",
    "> - Have they changed?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[-0.27759105,  0.38140574,  0.5914048 ],\n",
       "        [-0.11432791, -0.76287764,  0.48326385],\n",
       "        [ 0.74046695, -0.98022574, -0.4480464 ],\n",
       "        [ 0.7506665 ,  0.1215153 ,  0.08036764],\n",
       "        [-0.19381475,  0.26407424,  0.22773363],\n",
       "        [-0.11549103,  0.04309719,  0.03047322]], dtype=float32),\n",
       " array([ 0.        , -0.29006985,  0.2908665 ], dtype=float32),\n",
       " array([[ 0.6300874 ],\n",
       "        [-0.99999696],\n",
       "        [ 1.1647183 ]], dtype=float32),\n",
       " array([0.29056346], dtype=float32)]"
      ]
     },
     "execution_count": 259,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.get_weights()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - https://keras.io/api/optimizers/#available-optimizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optimizers comparison in GIF ‚Üí https://mlfromscratch.com/optimizers-explained/#adam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tesla's Neural Network Models is composed of 48 models trainned in 70.000 hours of GPU ‚Üí https://tesla.com/ai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1 Year with a 8 GPU Computer ‚Üí https://twitter.com/thirdrowtesla/status/1252723358342377472"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use Gradient Descent `SGD`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(layer=Input(shape=(6,)))\n",
    "model.add(layer=Dense(units=3, kernel_initializer='glorot_uniform'))\n",
    "model.add(layer=Dense(units=1, activation='linear'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### `compile()` the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='sgd', loss='mse', metrics=['mse'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `fit()` the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-02 11:37:31.584351: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X, y, epochs=500, verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predictions vs Reality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 1. Calculate the Predicted Accidents and\n",
    "> 2. Compare it with the Real Total Accidents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-02 11:37:35.804607: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>total</th>\n",
       "      <th>pred_init_1</th>\n",
       "      <th>pred_1_after_fit</th>\n",
       "      <th>pred_sigmoid</th>\n",
       "      <th>pred_linear</th>\n",
       "      <th>pred_tanh</th>\n",
       "      <th>pred_relu</th>\n",
       "      <th>pred_gsd</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>abbrev</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>AL</th>\n",
       "      <td>18.8</td>\n",
       "      <td>-257.937042</td>\n",
       "      <td>16.217836</td>\n",
       "      <td>1.0</td>\n",
       "      <td>18.319891</td>\n",
       "      <td>1.0</td>\n",
       "      <td>14.564026</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AK</th>\n",
       "      <td>18.1</td>\n",
       "      <td>-307.893494</td>\n",
       "      <td>18.597555</td>\n",
       "      <td>1.0</td>\n",
       "      <td>16.408112</td>\n",
       "      <td>1.0</td>\n",
       "      <td>16.471922</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AZ</th>\n",
       "      <td>18.6</td>\n",
       "      <td>-255.119736</td>\n",
       "      <td>20.028687</td>\n",
       "      <td>1.0</td>\n",
       "      <td>16.703562</td>\n",
       "      <td>1.0</td>\n",
       "      <td>17.129185</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AR</th>\n",
       "      <td>22.4</td>\n",
       "      <td>-259.586212</td>\n",
       "      <td>22.548859</td>\n",
       "      <td>1.0</td>\n",
       "      <td>18.824598</td>\n",
       "      <td>1.0</td>\n",
       "      <td>19.351301</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CA</th>\n",
       "      <td>12.0</td>\n",
       "      <td>-301.841125</td>\n",
       "      <td>10.325897</td>\n",
       "      <td>1.0</td>\n",
       "      <td>11.142485</td>\n",
       "      <td>1.0</td>\n",
       "      <td>9.687492</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        total  pred_init_1  pred_1_after_fit  pred_sigmoid  pred_linear  \\\n",
       "abbrev                                                                    \n",
       "AL       18.8  -257.937042         16.217836           1.0    18.319891   \n",
       "AK       18.1  -307.893494         18.597555           1.0    16.408112   \n",
       "AZ       18.6  -255.119736         20.028687           1.0    16.703562   \n",
       "AR       22.4  -259.586212         22.548859           1.0    18.824598   \n",
       "CA       12.0  -301.841125         10.325897           1.0    11.142485   \n",
       "\n",
       "        pred_tanh  pred_relu  pred_gsd  \n",
       "abbrev                                  \n",
       "AL            1.0  14.564026       NaN  \n",
       "AK            1.0  16.471922       NaN  \n",
       "AZ            1.0  17.129185       NaN  \n",
       "AR            1.0  19.351301       NaN  \n",
       "CA            1.0   9.687492       NaN  "
      ]
     },
     "execution_count": 264,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfsel['pred_gsd'] = y_pred\n",
    "dfsel.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'pred_sgd'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/24/tg28vxls25l9mjvqrnh0plc80000gn/T/ipykernel_4945/4078616456.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdfsel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtotal\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mdfsel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpred_sgd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mmse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniforge3/envs/dl/lib/python3.9/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   5485\u001b[0m         ):\n\u001b[1;32m   5486\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5487\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5488\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5489\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'pred_sgd'"
     ]
    }
   ],
   "source": [
    "mse = ((dfsel.total - dfsel.pred_sgd)**2).mean()\n",
    "mse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observe the numbers for the `weights`\n",
    "\n",
    "> - Have they changed?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[nan, nan, nan],\n",
       "        [nan, nan, nan],\n",
       "        [nan, nan, nan],\n",
       "        [nan, nan, nan],\n",
       "        [nan, nan, nan],\n",
       "        [nan, nan, nan]], dtype=float32),\n",
       " array([nan, nan, nan], dtype=float32),\n",
       " array([[nan],\n",
       "        [nan],\n",
       "        [nan]], dtype=float32),\n",
       " array([nan], dtype=float32)]"
      ]
     },
     "execution_count": 266,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.get_weights()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### View History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'val_loss'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/24/tg28vxls25l9mjvqrnh0plc80000gn/T/ipykernel_4945/3041766850.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'val_loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'model loss'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mylabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'loss'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxlabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'epoch'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'val_loss'"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use `ADAM`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(layer=Input(shape=(6,)))\n",
    "model.add(layer=Dense(units=3, kernel_initializer='glorot_uniform'))\n",
    "model.add(layer=Dense(units=1, activation='linear'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### `compile()` the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='mse', metrics=['mse'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `fit()` the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-02 11:39:05.132369: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2021-12-02 11:39:05.288460: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X, y, epochs=500, verbose=0, validation_split=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predictions vs Reality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 1. Calculate the Predicted Accidents and\n",
    "> 2. Compare it with the Real Total Accidents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-02 11:39:13.478370: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>total</th>\n",
       "      <th>pred_init_1</th>\n",
       "      <th>pred_1_after_fit</th>\n",
       "      <th>pred_sigmoid</th>\n",
       "      <th>pred_linear</th>\n",
       "      <th>pred_tanh</th>\n",
       "      <th>pred_relu</th>\n",
       "      <th>pred_gsd</th>\n",
       "      <th>pred_adam</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>abbrev</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>AL</th>\n",
       "      <td>18.8</td>\n",
       "      <td>-257.937042</td>\n",
       "      <td>16.217836</td>\n",
       "      <td>1.0</td>\n",
       "      <td>18.319891</td>\n",
       "      <td>1.0</td>\n",
       "      <td>14.564026</td>\n",
       "      <td>-1.061236</td>\n",
       "      <td>14.047134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AK</th>\n",
       "      <td>18.1</td>\n",
       "      <td>-307.893494</td>\n",
       "      <td>18.597555</td>\n",
       "      <td>1.0</td>\n",
       "      <td>16.408112</td>\n",
       "      <td>1.0</td>\n",
       "      <td>16.471922</td>\n",
       "      <td>29.059591</td>\n",
       "      <td>19.019171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AZ</th>\n",
       "      <td>18.6</td>\n",
       "      <td>-255.119736</td>\n",
       "      <td>20.028687</td>\n",
       "      <td>1.0</td>\n",
       "      <td>16.703562</td>\n",
       "      <td>1.0</td>\n",
       "      <td>17.129185</td>\n",
       "      <td>27.557539</td>\n",
       "      <td>17.933769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AR</th>\n",
       "      <td>22.4</td>\n",
       "      <td>-259.586212</td>\n",
       "      <td>22.548859</td>\n",
       "      <td>1.0</td>\n",
       "      <td>18.824598</td>\n",
       "      <td>1.0</td>\n",
       "      <td>19.351301</td>\n",
       "      <td>6.076072</td>\n",
       "      <td>15.823634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CA</th>\n",
       "      <td>12.0</td>\n",
       "      <td>-301.841125</td>\n",
       "      <td>10.325897</td>\n",
       "      <td>1.0</td>\n",
       "      <td>11.142485</td>\n",
       "      <td>1.0</td>\n",
       "      <td>9.687492</td>\n",
       "      <td>-1.993033</td>\n",
       "      <td>11.473425</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        total  pred_init_1  pred_1_after_fit  pred_sigmoid  pred_linear  \\\n",
       "abbrev                                                                    \n",
       "AL       18.8  -257.937042         16.217836           1.0    18.319891   \n",
       "AK       18.1  -307.893494         18.597555           1.0    16.408112   \n",
       "AZ       18.6  -255.119736         20.028687           1.0    16.703562   \n",
       "AR       22.4  -259.586212         22.548859           1.0    18.824598   \n",
       "CA       12.0  -301.841125         10.325897           1.0    11.142485   \n",
       "\n",
       "        pred_tanh  pred_relu   pred_gsd  pred_adam  \n",
       "abbrev                                              \n",
       "AL            1.0  14.564026  -1.061236  14.047134  \n",
       "AK            1.0  16.471922  29.059591  19.019171  \n",
       "AZ            1.0  17.129185  27.557539  17.933769  \n",
       "AR            1.0  19.351301   6.076072  15.823634  \n",
       "CA            1.0   9.687492  -1.993033  11.473425  "
      ]
     },
     "execution_count": 285,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfsel['pred_adam'] = y_pred\n",
    "dfsel.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15.479611437965968"
      ]
     },
     "execution_count": 286,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mse = ((dfsel.total - dfsel.pred_adam)**2).mean()\n",
    "mse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observe the numbers for the `weights`\n",
    "\n",
    "> - Have they changed?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[ 1.0261072 , -0.4412136 ,  0.24907677],\n",
       "        [ 0.3195029 ,  0.52382976,  0.9654815 ],\n",
       "        [ 0.81808597, -0.15780433, -0.2726606 ],\n",
       "        [ 0.6018822 , -0.24991553,  0.40426838],\n",
       "        [-0.5534314 ,  0.27578098,  0.18136519],\n",
       "        [ 0.4205665 , -0.3196814 , -0.04292154]], dtype=float32),\n",
       " array([0.14133202, 0.14390844, 0.1531938 ], dtype=float32),\n",
       " array([[0.31651676],\n",
       "        [0.57821   ],\n",
       "        [0.1873371 ]], dtype=float32),\n",
       " array([0.14417773], dtype=float32)]"
      ]
     },
     "execution_count": 288,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.get_weights()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### View History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use `RMSPROP`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(layer=Input(shape=(6,)))\n",
    "model.add(layer=Dense(units=3, kernel_initializer='glorot_uniform'))\n",
    "model.add(layer=Dense(units=1, activation='linear'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### `compile()` the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='rmsprop', loss='mse', metrics=['mse'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `fit()` the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-02 11:42:32.095026: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2021-12-02 11:42:32.183172: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X, y, epochs=500, verbose=0, validation_split=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predictions vs Reality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 1. Calculate the Predicted Accidents and\n",
    "> 2. Compare it with the Real Total Accidents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-02 11:42:52.907989: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>total</th>\n",
       "      <th>pred_init_1</th>\n",
       "      <th>pred_1_after_fit</th>\n",
       "      <th>pred_sigmoid</th>\n",
       "      <th>pred_linear</th>\n",
       "      <th>pred_tanh</th>\n",
       "      <th>pred_relu</th>\n",
       "      <th>pred_gsd</th>\n",
       "      <th>pred_adam</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>abbrev</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>AL</th>\n",
       "      <td>18.8</td>\n",
       "      <td>-257.937042</td>\n",
       "      <td>16.217836</td>\n",
       "      <td>1.0</td>\n",
       "      <td>18.319891</td>\n",
       "      <td>1.0</td>\n",
       "      <td>14.564026</td>\n",
       "      <td>-1.061236</td>\n",
       "      <td>9.824105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AK</th>\n",
       "      <td>18.1</td>\n",
       "      <td>-307.893494</td>\n",
       "      <td>18.597555</td>\n",
       "      <td>1.0</td>\n",
       "      <td>16.408112</td>\n",
       "      <td>1.0</td>\n",
       "      <td>16.471922</td>\n",
       "      <td>29.059591</td>\n",
       "      <td>16.033802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AZ</th>\n",
       "      <td>18.6</td>\n",
       "      <td>-255.119736</td>\n",
       "      <td>20.028687</td>\n",
       "      <td>1.0</td>\n",
       "      <td>16.703562</td>\n",
       "      <td>1.0</td>\n",
       "      <td>17.129185</td>\n",
       "      <td>27.557539</td>\n",
       "      <td>11.969419</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AR</th>\n",
       "      <td>22.4</td>\n",
       "      <td>-259.586212</td>\n",
       "      <td>22.548859</td>\n",
       "      <td>1.0</td>\n",
       "      <td>18.824598</td>\n",
       "      <td>1.0</td>\n",
       "      <td>19.351301</td>\n",
       "      <td>6.076072</td>\n",
       "      <td>10.892779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CA</th>\n",
       "      <td>12.0</td>\n",
       "      <td>-301.841125</td>\n",
       "      <td>10.325897</td>\n",
       "      <td>1.0</td>\n",
       "      <td>11.142485</td>\n",
       "      <td>1.0</td>\n",
       "      <td>9.687492</td>\n",
       "      <td>-1.993033</td>\n",
       "      <td>16.649256</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        total  pred_init_1  pred_1_after_fit  pred_sigmoid  pred_linear  \\\n",
       "abbrev                                                                    \n",
       "AL       18.8  -257.937042         16.217836           1.0    18.319891   \n",
       "AK       18.1  -307.893494         18.597555           1.0    16.408112   \n",
       "AZ       18.6  -255.119736         20.028687           1.0    16.703562   \n",
       "AR       22.4  -259.586212         22.548859           1.0    18.824598   \n",
       "CA       12.0  -301.841125         10.325897           1.0    11.142485   \n",
       "\n",
       "        pred_tanh  pred_relu   pred_gsd  pred_adam  \n",
       "abbrev                                              \n",
       "AL            1.0  14.564026  -1.061236   9.824105  \n",
       "AK            1.0  16.471922  29.059591  16.033802  \n",
       "AZ            1.0  17.129185  27.557539  11.969419  \n",
       "AR            1.0  19.351301   6.076072  10.892779  \n",
       "CA            1.0   9.687492  -1.993033  16.649256  "
      ]
     },
     "execution_count": 297,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfsel['pred_adam'] = y_pred\n",
    "dfsel.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "80.24908359983833"
      ]
     },
     "execution_count": 298,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mse = ((dfsel.total - dfsel.pred_adam)**2).mean()\n",
    "mse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observe the numbers for the `weights`\n",
    "\n",
    "> - Have they changed?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[-0.59858775,  0.8523246 ,  0.5146455 ],\n",
       "        [-0.10614524,  0.18939227, -0.08731039],\n",
       "        [-0.11281923,  0.10466293, -0.22494529],\n",
       "        [-0.23222369,  0.23848066,  0.5843881 ],\n",
       "        [ 0.29835042,  0.16147842, -0.18380637],\n",
       "        [ 0.02698331,  0.32116553, -0.37626266]], dtype=float32),\n",
       " array([-0.2997156 ,  0.31147367,  0.3063864 ], dtype=float32),\n",
       " array([[ 0.08885846],\n",
       "        [-0.54430515],\n",
       "        [-0.47985682]], dtype=float32),\n",
       " array([-0.31190482], dtype=float32)]"
      ]
     },
     "execution_count": 299,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.get_weights()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### View History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Does it take different times to get the best accuracy? Why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - https://keras.io/api/losses/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `binary_crossentropy`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `sparse_categorical_crossentropy`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `mean_absolute_error`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `mean_squared_error`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In the end, what should be a feasible configuration of the Neural Network for this data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Common Errors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The `kernel_initializer` Matters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The `activation` Function Matters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The `optimizer` Matters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Number of `epochs` Matters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The `loss` Function Matters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Number of `epochs` Matters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network's importance to find **Non-Linear Patterns** in the Data\n",
    "\n",
    "> - The number of Neurons & Hidden Layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://towardsdatascience.com/beginners-ask-how-many-hidden-layers-neurons-to-use-in-artificial-neural-networks-51466afa0d3e"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://playground.tensorflow.org/#activation=tanh&batchSize=10&dataset=circle&regDataset=reg-plane&learningRate=0.03&regularizationRate=0&noise=0&networkShape=4,2&seed=0.87287&showTestData=false&discretize=false&percTrainData=50&x=true&y=true&xTimesY=false&xSquared=false&ySquared=false&cosX=false&sinX=false&cosY=false&sinY=false&collectStats=false&problem=classification&initZero=false&hideText=false"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "- Mathematical Formula\n",
    "- Weights / Kernel Initializer\n",
    "- Loss Function\n",
    "- Activation Function\n",
    "- Optimizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What cannot you change arbitrarily of a Neural Network?\n",
    "\n",
    "- Input Neurons\n",
    "- Output Neurons\n",
    "- Loss Functions\n",
    "- Activation Functions"
   ]
  }
 ],
 "metadata": {
  "authors": [
   {
    "name": "Jes√∫s L√≥pez @sotastica"
   }
  ],
  "interpreter": {
   "hash": "a2b8701b642343483c5f5e717bcb0768c6b951acf38d76a6fc8ea01492bda71d"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
